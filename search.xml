<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[一些流水账(不定期更新)]]></title>
      <url>%2F2333%2F10%2F25%2Flife%2F</url>
      <content type="text"><![CDATA[今晚拖着疲倦的身体从实验室出来在操场上散步，靠近看台的地方有个人杵在那唱着郑钧的《回到拉萨》，很大声，也很自信。不过最终的效果就像一头海豹正在分娩农业机械。 --2017.03.11 对了，上午看到一个运营吐槽删除微博中的违规评论特别复杂。一个朋友评论说写JS脚本去处理，之前我遇到这种问题想到的是写python脚本。后来一想的确是直接在chrome的console里写JS便捷得多，于是尝试写了一下，基本就是找到元素匹配违规字符串，然后click()一下。不过一些边界条件没考虑，可能循环会出错。 --2017.02.27 今天凑巧看了一个吐槽大会的王刚剪辑，直接叹服，果然还是老演员厉害。活生生把讲段子演绎成了单口相声，动作与表情恰到好处，精准到每一个变化都在挠你的心窝子，还带有一口标准流利的美式英语，顺带知道了他当过第一期春晚的同声传译。这样一说来，我还是更加喜欢表现派演技的演员，他们总会有一套自己的理论并会不断完善它，表情，动作和语气就是他们的三板斧，至少能让我这个观众可以欣赏到完全的表演美学。这是体验派无法让我感知到的，当然也可能是我眼钝，虽然能轻松让我入戏，但无法觉察到他们在巧妙地演绎一个角色。真实而缺乏技巧，没错，我就是这么严格。 --2017.02.25 我常常自诩为我思维可以自由灵活地变通，但经过长久的自省后发现实则不然。说到自省每个人自省的状态都不同，有的人通过写日记，有的人通过夜半三更坐在床上陷入沉思，而我是发呆，在发呆中有三分之一的时间在自我反省，其他三分之二当然是真的在发呆。比如玩狼人杀时我常常就会认准一套逻辑走到消亡，而这逻辑常常剑走偏锋，听上去好又不好。思来想去，我发现我的思维像什么呢，就像我之前买的铁质台灯一样，看上去台灯支架上有一些弹簧和螺母可供调节，但是只是在一定的范围内，但你想把它扭转到另一个180度时，就像在叫一个腰椎间盘突出的人做腰部伸展运动。 --2017.02.17 今天发生了一件有趣的事。我和好朋友狗郑在一家自助烧烤店一边吃着东西，一边像一对基佬有说有笑着，旁边的一位服务员阿姨频频要给我们端东西调酱什么的。我们正疑问为什么阿姨对我们这么来神，阿姨笑着给我们递来酱，一边说道：“两个小伙子长得真帅气。”我们俩噗嗤一笑，我猜想着是不是阿姨想要把女儿介绍给我们，不过这种事情够我吹好一阵子了，即便不吹出来，那也算得上一件可以正经地树立自信心的事，当时的心情大概就像是吃第一口巧乐兹，刚咬到巧克力那层壳子(当然这不妨碍里面的巧克力更好吃)。 --2017.01.21 这些天算领悟到了。搞科研就像打麻将，我从来不熟悉打麻将。 --2017.01.17 今天在书中看到一句话，这句话是汽车大王亨利福特说的：“如果我当初通过问别人想要什么采取发明什么，他们只会告诉你他们想要跑得更快的马。”这句话所反应的跟乔布斯所做有惊人的雷同之处，作为一个产品经理，一个工程师去设计一款产品，太需要怀揣着自由意志，如果只是跟着用户的需求走，估摸着这个产品只算得上一个打磨得比较漂亮的工艺品。 --2017.01.16 他这个人是极度自恋的，这源于他内心的极度自卑。我们都在矛盾中去寻求生存，但是谁又能做到这么极致呢？这是常人难以臻至的境界。鲜明与出众的魅力也由此诞生。 --2017.01.13 若是把三观正不正当成衡量一部电影好坏的标准，那很多电影史上的佳片都会被这个标准拖下水。《沉默的羔羊》、《黑社会》这些电影可都没有宣扬世俗意义上的真善美，在我看来，三观的正与否是一部分观众的自尊心驱使去判定的，他们过于想用自己的精神利益和“政治正确”来禁锢导演的价值观，这是徒劳的。电影和导演可都没有义务去普及什么是好的三观，你若想把电影当作艺术去看，就不用想着把它变成宣传语，大字报。 --2017.01.03 今晚我和朋友看了大烂片《长城》。我之前曾许下承诺不去影院看7分以下的电影，这是我破的第二次戒，第一次是去看了6.9分的《美人鱼》，算是临界破戒。这次《长城》只有4.9分，我还是去看了，其中有三个原因，一是我看过张艺谋关于长城的采访，我对其诚恳的态度较有好感；二是他拍得《活着》在我心中有很高的地位，私以为超过《霸王别姬》；三是和朋友有个习惯吃完饭之后就是看电影，他直接推荐了此片。这三个原因中三最为重要，秉着这三种原因我顺利地看完了它，弊病的确挺多，前部分太过刻意卖弄中国元素，让我有种回到2018奥运会开幕式的错觉；服装过于花哨；人物刻画太过匆忙，完全被剧情捆死。不过其实也没那么不堪，特效很足，一部好莱坞流水线似的作品，剧情中规中矩，就是对中国文化的表现太过刻意，倘若是这样露骨地表达，外国人想必也不会感兴趣。仔细一想，的确很多事倘若去隐忍克制地表达，往往能突出更多的美感。当然这只局限于本身美好的事物，丑陋的人或事就算犹抱琵琶半遮面，其他人也想把露出的那半边脸也遮上吧。 --2016.12.31 窦唯和译乐队合作的新专《间听监》发了，赶紧试听了一下。采样了很多国产谍战电影的台词原声，有部分粉丝很不理解为什么窦会突然关注起谍战和阴谋论的元素，说好的成仙呢。其实，窦在音乐上一直是先锋的，也可以理解带有些许成仙的元素，但窦在作曲背景上一直是紧紧和当今世界发生的事联系起来的，包括之前殃金咒的创作是源于都在报纸上看到一条河中因工业污染漂了很多死猪的新闻，天宫图的背景中国天宫一号发射，这样一看，会发现窦在骨子里越来越像中国古代的文人。这样的人在现代可真算得上罕见了哈哈。 --2016.12.26 “七孔流血是七孔流血，死是死，你千万不要弄混淆”。一想来真是富有哲理的一句话，如果把两种概念根据常识却不加思考地联系在一起，在特殊的人或事物上往往是不奏效的。甚至就是一般的情况下，你也不能一股脑地从A走向B，A到B这条线是人画的，可不是上帝画的。(又温习一遍回魂夜，周星驰怎么就这么鬼才呢，不枉我爱他这么多年。) --2016.12.25 总算发现一本算比较详细介绍Spark内核源码的书了，虽然只是gitbook的形式。https://spark-internals.books.yourtion.com -- 2016.12.23 不定时会发现朋友圈部分人发着一段一段“青春伤感文学”的文字，我姑且这么称呼它的原因是它大多时候包含但不仅限于这几个元素：情爱，酒，民谣，怀念。我作为严肃文学的“爱好者”，并不是想要把异己拿出来批判一番，但是总是感觉这种文字看似说了一大段，但意味不明，它仿佛要表达很多，但多的又不知道想表达什么。我估摸着大概就是写给自己看的，既然是自己看我也便提不出什么要求，但还是会夹带些许好奇，这些好奇我倘若是都放在技术问题上，我想自己还会有更多的进步。 --2016.12.17 Leetcode的hard难度不愧为hard难度哈，虽然有思路但终究是不能通过全部test，操蛋得很。忍不住看看讨论区大神们的做法：我刚才在写什么?? forget it。 --2016.12.16 lab杨和paper明对我拿吹风机吹脚这件事似乎抱有一定成见，我想说的是吹脚并不是因为我脚板底有毛，而是上面有水，如此一说这个行为看上去就很合理。毕竟吹风机是用来把水吹干，而不是吹掉毛发的，所以不会仅局限于头部，我想往后我还会将它应用于更多的地方。--2016.12.14]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[用owncloud搭建你自己的云]]></title>
      <url>%2F2017%2F01%2F11%2Fowncloud%2F</url>
      <content type="text"><![CDATA[what is owncloudownCloud是一个开源免费的云端文件储存分享平台，它支持包括网页、PC客户端、移动App在内的所有终端使用。为何要搭私人网盘服务？如今很多网盘一个一个倒下，用U盘存储服务风险较大，而且很不方便。我曾经买过很多U盘，但有一天我发现，我买它们有什么意义呢？它们很可爱，但它们可爱的时间总是很短暂。一个月内，它们必然卷带着资料消失地一干二净。所以自己动手吧，丰衣足食。我不是鼓励大家有小农经济的思想，而是寻求一种“这些都在我掌控之中”的感觉。而且要知道，其实很多富人都会在自己后院里种点蔬菜什么的，我想大概是一个道理。 你要怎么做硬件设施你可以选择购买物理服务器，也可以购买靠谱的云服务器。云服务比如阿里云，亚马逊云什么的，租一个月花费也不高，而且可以顺便搭建自己的VPN服务。我自己是在物理服务器上搭的，因为我手里掌控着实验室楼下的几台服务器，平时跑跑实验，但大部分时间他们都是空闲的，想想有点浪费资源。虽然之前在上面搭过一些小服务，但是我还可以利用的一个有效资源就是内网。在内网中传输文件众所周知是极其迅速的，这也是我想在实验室服务器上搭云盘的根本原因。之前我和实验室同学要是传递一些文件的话，如果通过第三方服务比如QQ就会很慢，通过U盘又会很麻烦。当然通过scp命令是可以通过内网的，但是windows系统的话便无能为力。所以这个云盘的其中一个重要作用就是文件中转站，同学共享一个大型文件，其他人都可以很快下载。 软件配置当然服务器得是Linux系统了，这是前提条件，以下配置内容适用于centos发行版。还有服务器需要安装apache tomcat服务。第一步，安装必要的php扩展。sudo yum install php-mysql php-json php-xml php-mbstring php-zip php-gd curl php-curl php-pdo第二步，为owncloud配置mysql数据库。当然也可以不配，我就没有配，如果不用mysql数据库的话，owncloud会采用默认的SQLite。第三步，下载安装owncloud包。centos可以直接通过yum自动安装：sudo yum install owncloud,一般会带着两个依赖包，一共三个软件包。第四步，安装之后，owncloud的网页文件会存储在/var/www/html/owncloud文件夹里，配置文件在/etc/httpd/conf.d文件夹内。这时候启动http服务，sudo service httpd start,在浏览器访问ip/owncloud，会看到这个界面：,但当你注册新用户的时候会出现一系列权限问题。第五步：解决权限问题，这个我在网上搜了很多，但都无效。可能是解决方法跟不上新版本的原因，后来发现，首页有个链接指向如何配置权限的官方文档，权限配置链接。运行以下脚本就可以：12345678910111213141516171819202122232425262728293031323334353637#!/bin/bashocpath='/var/www/owncloud'htuser='www-data'htgroup='www-data'rootuser='root'printf "Creating possible missing Directories\n"mkdir -p $ocpath/datamkdir -p $ocpath/assetsmkdir -p $ocpath/updaterprintf "chmod Files and Directories\n"find $&#123;ocpath&#125;/ -type f -print0 | xargs -0 chmod 0640find $&#123;ocpath&#125;/ -type d -print0 | xargs -0 chmod 0750printf "chown Directories\n"chown -R $&#123;rootuser&#125;:$&#123;htgroup&#125; $&#123;ocpath&#125;/chown -R $&#123;htuser&#125;:$&#123;htgroup&#125; $&#123;ocpath&#125;/apps/chown -R $&#123;htuser&#125;:$&#123;htgroup&#125; $&#123;ocpath&#125;/assets/chown -R $&#123;htuser&#125;:$&#123;htgroup&#125; $&#123;ocpath&#125;/config/chown -R $&#123;htuser&#125;:$&#123;htgroup&#125; $&#123;ocpath&#125;/data/chown -R $&#123;htuser&#125;:$&#123;htgroup&#125; $&#123;ocpath&#125;/themes/chown -R $&#123;htuser&#125;:$&#123;htgroup&#125; $&#123;ocpath&#125;/updater/chmod +x $&#123;ocpath&#125;/occprintf "chmod/chown .htaccess\n"if [ -f $&#123;ocpath&#125;/.htaccess ] then chmod 0644 $&#123;ocpath&#125;/.htaccess chown $&#123;rootuser&#125;:$&#123;htgroup&#125; $&#123;ocpath&#125;/.htaccessfiif [ -f $&#123;ocpath&#125;/data/.htaccess ] then chmod 0644 $&#123;ocpath&#125;/data/.htaccess chown $&#123;rootuser&#125;:$&#123;htgroup&#125; $&#123;ocpath&#125;/data/.htaccessfi 效果首页显示： 下载速度：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从源码看Spark的Job执行]]></title>
      <url>%2F2016%2F12%2F31%2Fspark-schduler%2F</url>
      <content type="text"><![CDATA[OverviewSpark现在已经更迭到2.0了，之前基本都在1.4版本的代码上涂涂改改，着实有点跟不上Spark更新的速度了，最近大概看了Spark2.0相比于1.4更新的点，外壳方面增加了一个SparkSession的类，用户不再显式调用SparkContext了，初看还是挺别扭的。在1.6中加了统一内存管理，比以往更加细粒度的管理内存了，因为内存界限不再是static congigure。不过之前看到别人的文章在这方面提到过一个弊病，那篇文章的名字叫《向Spark开炮：1.6版本问题总结与趟坑》，名字听着很具有雄性的震慑力，看了之后发现开的炮还是有理有据的，在统一内存管理模块，该文提到：默认情况下executor-memory的75%会拿出来供存储内存和运行内存使用（各占一半），存储内存和运行内存可以相互借用，避免了浪费的情况，有效的提高了内存的使用率。听起来是挺好的，但是运行任务的时候发现了一个问题：存储内存把所有内存占完了，Shuffle的时候它再慢慢从内存中剔除，多浪费了一次加载和剔除的时间。为什么会发生这种情况呢？因为Cache操作发生在Shuffle操作之前，这个时候运行内存是空闲的，它就顺势把运行内存给占完了，有点类似贪心的意味，等进行Shuffle操作的时候，已无内存可用，只能要求存储内存归还。还有在看Spark1.6的shuffle部分代码时，发现ExternalSorter在插入记录时，内存预估直接决定是否将buffer持久化到磁盘上，而没有了是否要spill的判断，后来看了一下官方的api，果不其然1.6版本移除了这个配置，其实当时就觉得这个功能挺鸡肋的，用户除非有十足的把握shuffle内存足够，不然谁会愿意将spill设置为true，等着程序报错呢。这种判断应该完全交给系统本身去做，虽然系统中也是预估，但总比人为推测靠谱一点。还有一个大一点的变化是：shuffle版本的不断更迭。在1.5版本时开始Tungsten钨丝计划，引入UnSafe Shuffle优化内存及CPU的使用，大致思想就是将对象存放在堆外，一个很大的好处就是减少GC。在1.6中将Tungsten统一到Sort Shuffle中，实现自我感知选择最佳Shuffle方式到最近的2.0版本，Hash Shuffle已被删除，所有Shuffle方式全部统一到Sort Shuffle一个实现中。到头来还是沿用了mapreduce始祖hadoop的方式，哈哈。下图是spark shuffle版本的演进： 当然新版本更新的特性要比我说得多得多，我只是拿我自己看到的一些点说一下.上面这些并不算赘述，恰恰是想说Spark是一个非常有活力的系统，它的很多特性都在保持着探索更佳的状态，但是不可忽略的一点是，它的一些核心思想不会变，所以看老版本的代码并不会让你觉得在浪费时间，而会让你在看到新版本的时候有更多的感悟。之前因为Shuffle部分的代码看得比较多，也改了挺多。所以在此想总结点自己不太熟悉的东西，关于Spark的job提交与执行的机制（代码来自于Spark-1.6版本）。在此盗一张关于调度的架构图：（此图来源于github地址） 关于Jobjob-&gt;task的运行过程job的提交需要action算子的触发，一个作业可以包含多个job。job会从后往前遍历RDD，以shuffle dep来划分stage，stage拆分为一组Task然后提交。driver端调用顺序 finalRDD.action()=&gt; sc.runJob() // generate job, stages and tasks=&gt; dagScheduler.runJob()=&gt; dagScheduler.submitJob()=&gt; DAGSchedulerEventProcessLoop.post(JobSubmitted)=&gt; dagSchedulerEventProcessLoop.doOnReceive(case JobSubmitted())=&gt; dagScheduler.handleJobSubmitted()=&gt; finalStage = newStage()=&gt; mapOutputTracker.registerShuffle(shuffleId, rdd.partitions.size)=&gt; dagScheduler.submitStage()=&gt; missingStages = dagScheduler.getMissingParentStages()=&gt; dagScheduler.subMissingTasks(readyStage) // add tasks to the taskScheduler=&gt; taskScheduler.submitTasks(new TaskSet(tasks))=&gt; schedulableBuilder(fifo or fair).addTaskSetManager(taskSet) // send tasks=&gt; CoarseGrainedSchedulerBackend.reviveOffers()=&gt; driverActor ! ReviveOffers=&gt; CoarseGrainedSchedulerBackend.makeOffers()=&gt; CoarseGrainedSchedulerBackend.launchTasks()//之间有通信=&gt; foreach task CoarseGrainedExecutorBackend(executorId) ! LaunchTask(serializedTask) 解释一下最后的分配TasksparkDeploySchedulerBackend接收到taskSet后，会通过自带的DriverActor将serialized tasks发送到调度器指定的worker node上的CoarseGrainedExecutorBackend Actor上。 Worker 端接收到 tasks 后，执行如下操作 coarseGrainedExecutorBackend ! LaunchTask(serializedTask)=&gt; executor.launchTask()=&gt; executor.threadPool.execute(new TaskRunner(taskId, serializedTask)) executor将task包装成taskRunner，并从线程池中抽取出一个空闲线程运行task。一个CoarseGrainedExecutorBackend进程有且仅有一个executor对象。一个应用程序就会启动一个新的backend进程。 Executor接收到序列化的task后，先反序列化，然后运task得到其执行结果directResult，这里的directResult一般是ShuffleMapTask或者是ResultTask。ShuffleMapTask返回的是MapStatus，包含了BlockManagerId和block的大小；还有一种情况是ResultTask，是func在partition上的执行结果。这个结果要送回到driver那里，但是通过Actor发送的数据包不易过大，如果result比较大（比如groupByKey的result先把result存放到本地的“内存＋磁盘”上，由blockManager来管理，只把存储位置信息（indirectResult）发送给driver，driver需要实际的result的时候，会通过HTTP去fetch。如果result不大（小于spark.akka.frameSize=10MB），那么直接发送给driver。Driver收到task的执行结果result后会进行一系列的操作：首先告诉taskScheduler这个task已经执行完，然后去分析result。由于result可能是indirectResult，需要先调用blockManager.getRemoteBytes()去fetch实际的result。得到实际的executor执行的result后，需要分情况分析，如果是ResultTask的result，那么可以使用ResultHandler对result进行driver端的计算（比如 count()会对所有ResultTask的result作sum），如果result是ShuffleMapTask的MapStatus，那么需要将MapStatus（ShuffleMapTask输出的FileSegment的位置和大小信息）存放到mapOutputTrackerMaster中的mapStatuses数据结构中以便以后reducer shuffle的时候查询。在ShuffleMapTask执行完获得MapStatus的时候，在DAGScheduler类中handleTaskCompletion的方法中：12345678910111213141516case smt: ShuffleMapTask =&gt; val shuffleStage = stage.asInstanceOf[ShuffleMapStage] updateAccumulators(event) val status = event.result.asInstanceOf[MapStatus] val execId = status.location.executorId logDebug("ShuffleMapTask finished on " + execId) if (failedEpoch.contains(execId) &amp;&amp; smt.epoch &lt;= failedEpoch(execId)) &#123; logInfo(s"Ignoring possibly bogus $smt completion from executor $execId") &#125; else &#123; shuffleStage.addOutputLoc(smt.partitionId, status) &#125; 。。。 mapOutputTracker.registerMapOutputs( shuffleStage.shuffleDep.shuffleId, shuffleStage.outputLocInMapOutputTrackerFormat(), changeEpoch = true) 下一个stage如何获取Shuffle文件reducer首先要知道parent stage中ShuffleMapTask输出的FileSegments在哪个节点。这个信息在ShuffleMapTask完成时已经送到了driver的mapOutputTrackerMaster，并存放到了mapStatuses:HashMap(一种特殊的hashmap)里面，给定stageId，可以获取该stage中ShuffleMapTasks生成的FileSegments信息Array[MapStatus]，通过Array(taskId)就可以得到某个task输出的 FileSegments位置（blockManagerId）及每个FileSegment大小。这里涉及的类有点多，在此就不一一赘述。关于reduce要怎么读reduce文件：12345SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context) .read() .asInstanceOf[Iterator[(K, C)]]。。。mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition) 这里Spark-1.6和1.4版本调用的方法不太一样，但思想大致相同。reduce Task读取shuffle文件时还是根据partition id来规定起始和结束的partition，一个Reduce Task只读取shuffle文件中的一部分，也就是其中的一部分key。startPartition和endPartition会相应的映射为block id便于读取。真正根据block id读取远程文件的代码在ShuffleBlockFetcherIterator的sendRequest方法里，挺难找的。。由于各种迭代器加上scala的语法糖12345678910111213141516171819202122val address = req.address shuffleClient.fetchBlocks(address.host, address.port, address.executorId, blockIds.toArray, new BlockFetchingListener &#123; override def onBlockFetchSuccess(blockId: String, buf: ManagedBuffer): Unit = &#123; // Only add the buffer to results queue if the iterator is not zombie, // i.e. cleanup() has not been called yet. if (!isZombie) &#123; // Increment the ref count because we need to pass this to a different thread. // This needs to be released after use. buf.retain() results.put(new SuccessFetchResult(BlockId(blockId), address, sizeMap(blockId), buf)) shuffleMetrics.incRemoteBytesRead(buf.size) shuffleMetrics.incRemoteBlocksFetched(1) &#125; logTrace("Got remote block " + blockId + " after " + Utils.getUsedTimeMs(startTime)) &#125; override def onBlockFetchFailure(blockId: String, e: Throwable): Unit = &#123; logError(s"Failed to get block(s) from $&#123;req.address.host&#125;:$&#123;req.address.port&#125;", e) results.put(new FailureFetchResult(BlockId(blockId), address, e)) &#125; &#125; 最后以规定的反序列化方式来读取KV对装载进iterator返回给ShuffleReader完成后续的操作。 总结代码贴得有点多，可能不便阅读，但是Spark里面一个方法里经常完成了许多动作，因为考虑的因素太多，所以写得很细致，很多情况下一个方法里就包含了很多判断和容错操作，全靠文字描述可能要花很大篇幅。附上代码也便于以后自己的回顾，在最短的时间找到我想看到的部分。在开篇说不提shuffle，还是提到了一部分，毕竟它也是job执行的重要环节，不过也只介绍了个大概。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[谈回溯法]]></title>
      <url>%2F2016%2F12%2F10%2Fbacktrack%2F</url>
      <content type="text"><![CDATA[前言起先刷到LeetCode中的一题Combination Sum，题目要求是求一组数中如何搭配求和能得到目标值，也就是求目标值的所有解。该题地址，其实就是在一个组合中求所有解集，所以都可以归为一种算法原型，也就是要说的回溯法。 介绍一下它概念回溯算法实际上一个类似枚举的搜索尝试过程，主要是在搜索尝试过程中寻找问题的解，当发现已不满足求解条件时，就“回溯”返回，尝试别的路径。 回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。(许多复杂的，规模较大的问题都可以使用回溯法) 基本思想在包含问题的所有解的解空间树中，按照深度优先搜索的策略，从根结点出发深度探索解空间树。当探索到某一结点时，要先判断该结点是否包含问题的解，如果包含，就从该结点出发继续探索下去，如果该结点不包含问题的解，则逐层向其祖先结点回溯。（其实回溯法就是对隐式图的深度优先搜索算法）。 解题三步骤1）定义问题的解空间如0-1背包问题，当n=3时，解空间是(0,0,0)、(0,0,1)、(0,1,0)、(0,1,1)、(1,0,0)、(1,0,1)、(1,1,0)、(1,1,1)。1代表选择该物品，0代表不选择该物品2）确定易搜索的解空间结构3）以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索 算法思路我暂时只介绍我所遇到的情况，子集树和排列树。还有一个非子集和排列的类别暂且还未遇到，倘若以后碰到，会再更新。关于N皇后问题，其实就是非子集加排列类型的问题。其中大部分搜索解集的问题大都可以套下面的算法模板，可能会做一小部分更改。这里我只讨论递归的算法框架，原因很简单，递归的代码更精简，更优雅。 子集树典型例子：装载问题；0-1背包问题；最大团问题;求所有子集123456789101112131415161718void backtrack(int t)//t是当前层数 &#123; if(t&gt;n)//需要判断每一个元素是否加入子集，所以必须达到叶节点，才可以输出 &#123; output(x); &#125; else &#123; for(int i=0;i&lt;=1;i++)//子集树是从集合S中，选出符合限定条件的子集，故每个元素判断是（1）否（0）选入即可（二叉树），因此i定义域为&#123;0,1&#125; &#123; x[t]=i;//x[]表示是否加入点集，1表示是，0表示否 if(constraint(t)&amp;&amp;bound(t))//constraint(t)和bound(t)分别是约束条件和限定函数 &#123; backtrack(t+1); &#125; &#125; &#125; &#125; 其中的约束条件和限定函数负责“剪枝”功能，在我刷的关于回溯法的题中，大部分是去除掉关于重复的情况。 排列树典型例子：全排列问题12345678910111213141516171819void backtrack(int t)//t是当前层数 &#123; if(t&gt;n)//n是限定最大层数 &#123; output(x); &#125; else &#123; for(int i=t;i&lt;=n;i++)//排列树的节点所含的孩子个数是递减的，第0层节点含num-0个孩子，第1层节点含num-1个孩子，第二层节点含num-2个孩子···第num层节点为叶节点，不含孩子。即第x层的节点含num-x个孩子，因此第t层的i，它的起点为t层数，终点为num，第t层（根节点为空节点，除外），有num-t+1个亲兄弟，需要轮num-t+1回 &#123; swap(x[t],x[i]);//与第i个兄弟交换位置，排列树一条路径上是没有重复节点的，是集合S全员元素的一个排列，故与兄弟交换位置后就是一个新的排列 if(constraint(t)&amp;&amp;bound(t))//constraint(t)和bound(t)分别是约束条件和限定函数 &#123; backtrack(t+1); &#125; swap(x[i],x[t]); &#125; &#125; &#125; LeetCode中的回溯法Combination Sum123456789101112131415161718192021222324252627public class Solution &#123; public List&lt;List&lt;Integer&gt;&gt; combinationSum(int[] candidates, int target) &#123; List&lt;List&lt;Integer&gt;&gt; results = new ArrayList&lt;List&lt;Integer&gt;&gt;(); List&lt;Integer&gt; current = new ArrayList&lt;Integer&gt;(); Arrays.sort(candidates); back_track(results,current,candidates,target,0); return results; &#125; public static void back_track(List&lt;List&lt;Integer&gt;&gt; results,List&lt;Integer&gt; current,int[] candidates,int target,int floor)&#123; if(target == 0)&#123; results.add(new ArrayList&lt;Integer&gt;(current)); &#125;else if(target&lt;0)&#123; return; &#125;else &#123; for (int i = floor; i &lt; candidates.length; i++) &#123; int value = candidates[i]; current.add(value); back_track(results, current, candidates, target - value, i); //remove the last one current.remove(current.size() - 1); &#125; &#125; &#125;&#125; Permutations123456789101112131415161718192021222324252627282930313233 public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) &#123; List&lt;List&lt;Integer&gt;&gt; results = new ArrayList&lt;List&lt;Integer&gt;&gt;(); List&lt;Integer&gt; tempResult = new ArrayList&lt;Integer&gt;(); pailie(results,tempResult,nums,0); //System.out.println(results.size()); return results; &#125; public static void pailie(List&lt;List&lt;Integer&gt;&gt; results,List&lt;Integer&gt; tempReult,int[] nums,int floor)&#123; if(floor&gt;nums.length-1)&#123; // System.out.println(Arrays.toString(nums)); for(int num:nums)&#123; tempReult.add(num); &#125; results.add(new ArrayList&lt;Integer&gt;(tempReult)); tempReult.clear(); &#125;else&#123; for(int i = floor;i&lt;nums.length;i++)&#123; swap(nums,floor,i); pailie(results,tempReult,nums,floor+1); swap(nums,i,floor); &#125; &#125; &#125; public static void swap(int[] nums,int i,int j)&#123; int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; &#125;&#125; 有时间再说下N皇后问题]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[关于伪工作]]></title>
      <url>%2F2016%2F12%2F09%2Ffake-work%2F</url>
      <content type="text"><![CDATA[记得之前的某个时间点听到一个本科的小伙伴在朋友圈发表了关于伪工作的看法。当时他大概意在抨击一些产品经理，大致意思是某些产品觉得自己好像为用户考虑了很多，做了很多，其实说真的还不如什么都不干。我对产品经理是不熟悉的，就在之前实习过的那半年里，也只是跟运维打交道，有趣的是一般产品经理会主动找程序员沟通或者变更需求，那些时间好像我跑得比运维更勤，几次我都认为某些需求定的不太合理，不过幸运的是运维大部分都采纳了。不过根据网络上的舆论情况看，很多猿都会被需求和业务逼得苦不堪言，我也是在实习那段时间领悟到这点，不能一直被业务所束缚，因为业务的更迭会让一个猿写不到新的东西，学不到新的知识和技巧，当你做一件事长期不用动脑子，那这件事通常会让人身心俱疲甚至会让你处于危险的境地，源于它的重复性或者说是机械性。但是猿刚进门槛时，都或多或少接触到业务，如果你厌倦这些，就必须的得加速自己的技术成长，学会徒手造轮子，就我个人来言，我正在往这方面努力，研究生阶段也给了我更多的时间去了解和学习一个系统内部的子模块是如何实现，算是众多读研弊病中的反而较明显的优势。 至于伪工作，就是明明做的是毫无意义或者毫无价值的事情却觉得自己努力工作了一天，甚至会觉得内心非常充实。这对个人的心理上来说，不算缺点，因为至少这种感觉是让你内心舒适的。但在个人技能成长上来说，它是糟糕的(我希望在此刻加上川普的ok手势辅助加上一些强烈的情感)，可以说甚至是比打了长时间游戏，看了很多剧更糟糕，因为后者不光能放松娱乐，而且或多或少会有人负罪感能够更好的投入有价值的工作，而伪工作会让你一直处于精神麻痹的状态以为自己在认真工作，并且这种状态会让你无法自拔，让你上瘾，让你在日日夜夜中靠着这些伪工作荒唐的总结自己，最终你什么也没干成，或者什么成就都不会眷顾你。除非你运气足够好。 雷军是我很佩服的一个人，他说过“你不要用战术上的勤奋,掩盖战略上的懒惰”。这句话说得简直极对，我们在说人懒惰，说得是什么懒惰。其实大部分社会人在肉体上并不懒惰，还可以说得上是极度勤奋，我曾不止一次表示过我觉得汉族算是全球中最温顺勤奋的民族，这种普适性我觉得不算荒唐。更多人是在精神上懒惰，懒于改变，懒于接收新的事物，新的知识，新的土地，仅仅操守这那一亩三分地辛勤地耕作。九十年代香港导演算得上风生水起，新世纪初港片没落，大陆市场崛起，很多导演捧着他们那块旧土地继续汲取着快要枯竭的养分，而乐于改变，具有战略眼光的导演早已瞄准这个市场，大力与大陆的演员，电影公司合作，比如我的偶像周星驰，活脱脱的求新的人，曾不止一次表示过，拍电影一定要拍新的东西，结果就是他自导自演以来，没有一部电影题材是相似的。这里还可以顺便提及这样一种人，固守着老旧的资本，我不能把他们直接定性为江郎才尽，原因是他们可能干脆就没有接受新事物的欲望，但这样的行为必然是他们的自由。但若是还要借此抨击社会，甚至抨击所有人类，那我就要劝他们认真想想事实是不是这样，要知道如果你的引力足够大，外物围绕着你转倒还可以理解，但现在还没有可以让人瞬间变成巨型黑洞的技术。 伪工作的始作俑者便是人性中的精神惰性，这种惰性使人丧失战略甚至是计划，使人迷恋于忙碌，使人透支娱乐和休闲的时间。有趣的是，这种惰性一定意义上正是在把人往一刻也不停歇的机器人方向引导。机器人没有思想，没有自己的见解，他只需要按照命令，按照流程走，伪工作就是这样。人大多时候需要做到逆人性，人性要你做什么，你就要反问他些什么。我不想把这些话说得太说教，因为我没有权利教导人们，一个很大的原因是我本人就时常受到伪工作的荼毒，用这种严重的词汇不是说我受到了多大的损害，毕竟伪工作这东西就是自己造成的，而是说明一切伪的事物都是有毒的，而且有的毒素很强。我始终告诫自己要身处于真实的境地，表达真实的想法，做真实的事情。倘若为了自欺欺人，那伪工作大可有施展的舞台。这事就像演戏，但我从来不鼓吹人生如戏，全靠演技，一切优秀的演员演戏目的各不相同，有的为了让观众欣赏到好的作品，有的享受演戏，体验不同的人生，但他们生活中都是最真实的自己，倘若非要在荧幕之外也表演一番，那他就不是演员，而是明星，是德艺双馨的艺术家。迂回这么多，那如何纠正伪，工作前询问自己为什么这样做，我这样是不是在工作，还是处于一个不真实的目的去工作，更高的一层是如何更高效地工作，有没有更聪明的办法，这是为了优雅地将工作与生活分开，否则你将把所有的时间空耗在无意义的工作学习之中，当然也有人不喜欢享受生活，他们热衷于输出生产力并乐在其中，那他们也可以通过此种改变在短暂的时间做更多的事情。说到这里，我并不是想让你的工作复杂度变得更高，而是用短时间的改革来换来更高的效率和意义，国家和公司都是这么干的。当然这仅局限于工作，生活不需要这么痛苦，生活是随性的，但再随性，也不要抛弃了真实，营造出一个“伪生活”。 所以，高效地、怀揣着想法学习与工作吧，这句话最主要的还是送给自己。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[摸索性重写Spark的groupByKey(shuffle部分)]]></title>
      <url>%2F2016%2F12%2F02%2FgroupByKey-design%2F</url>
      <content type="text"><![CDATA[一开始实现这个特性的时候，我并不能预见其性能是否可以优化，但是不可以不尝试。 我姑且把它叫做工程师的调性. 现有的工作与策略上面的前言虽说出了口，却又觉得中二，给人一种鸣人站在所有boss面前嚎着：“这就是我的忍道！”的既视感。但总觉得不得不说出来，而且是认真的那种。尝试嘛，是一个可以让人投入他所有的激情和年华在一件事上的词，此事未必带来较大的意义，但此事在经过尝试给人以信念后，会让他这一生变得更加有趣，甚至是伟大。如果有人将此作为自己做任何事的方法论，那我必然是无比佩服他的。这些话不是我刻意在技术笔记中加上一些鸡汤（对于鸡汤这种事，我并不是向大多数网友那样厌恶，我觉得能在理性的范围内给人带来正能量的东西，它绝对差不到哪里去），恰恰是我在生活中遇到一些人和事，我需要把我的所想敞开在这儿，至于为什么不单独写，那纯属是因为我懒，写不了那么大篇幅的。当然，倘若以后心血来潮，那也保不准挤出很大段文字来谈谈我的柔情似水。 最近Deca要在论文中加入VST拆解的思想和实现。之前包括缓存RDD这类运行中定长的数据拆解已经实现，但是shuffle buffer这一类对象中存在很多变长的成员，因为在shuffle过程中，reduceBykey尚能拆解，但比如groupByKey这个算子，它是将相同key的所有value聚合起来,也就一个key后面跟一串value，在shuffle运算过程中这个KV对的V是变长的，值是不确定的。之前我们所做的工作就是对Spark应用中的对象进行拆解，转换成基本的原生类型，获取他们的类型，这样之后就可以把它们写成字节数组的形式。 Spark-1.4里groupByKey在shuffle write端可以利用到堆外的内存，也就是tungsten-sort，所有的数据都会写在堆外并在堆外排序，但是shuffle-read端Spark默认还是用的HashShuffleReader,所有的聚合操作都在堆内完成，这个我们已经实现了read端的堆外版本，聚合操作运行在堆外。大致介绍下原理，这里就用到了VST拆解的原理，我们知道shuffle read端读出来的(K,C)对的基本类型，于是先实现了一个简易的map(UnsafeUnfixedWidthAggregationFlintMap),嗯名字略长。。这是一个针对系统的定制的map，也是用到了Hash原理，不过所有操作都是在堆外进行。这个map用于存储key和valueAddress，这个valueAddress是一个long型值，我们会将K对应的一组Value在堆外开辟一片疆土用于存储他们，当然每次新来value时我们会检查是否扩容，若扩容会改变这块疆土(堆外空间)的起始地址，因为涉及到内存的拷贝，所以map中的valueAddress就是这块存储区域的初始偏移地址。valueAddress指向的存储区域结构为： 让groupByKey也有mapSideCombine策略的依据大家都知道的是reduceByKey是具有mapSideCombine特性的： 123456789101112131415161718/** Write a bunch of records to this task's output */override def write(records: Iterator[Product2[K, V]]): Unit = &#123; val iter = if (dep.aggregator.isDefined) &#123; if (dep.mapSideCombine) &#123; dep.aggregator.get.combineValuesByKey(records, context) &#125; else &#123; records &#125; &#125; else &#123; require(!dep.mapSideCombine, "Map-side combine without Aggregator specified!") records &#125; for (elem &lt;- iter) &#123; val bucketId = dep.partitioner.getPartition(elem._1) shuffle.writers(bucketId).write(elem._1, elem._2) &#125;&#125; 这是HashShuffleWriter的一段代码，mapSideCombine参数会导致Map的结果调用aggregator，在写入record的时候，会判断mapSideCombine这个值，若其为真，则在shuffle的map端提前做聚合，然而这只适用于value为单个值，因为其长度和类型不变，若像groupByKey value为一组值时，这是value长度是会变化的，这对Unsafe是无法支持的。这时候有人谁说，那堆内可不可以实现mapSideCombine的groupByKey，答案是不可以，这里我看到了一段官方在代码中的注释 12345678 * Note: This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]] * or [[PairRDDFunctions.reduceByKey]] will provide much better performance. * * Note: As currently implemented, groupByKey must be able to hold all the key-value pairs for any * key in memory. If a key has too many values, it can result in an [[OutOfMemoryError]]. */def groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] = self.withScope &#123; 天呐，会导致OOM。但我这里还有点疑问，在map端groupByKey会引起大量相同的key的value在内存中驻留，但是这个操作在write端也会导致内存膨胀哎，略带不解。既然这样，为何还要实现堆外版本的，我的解释是这样的：堆外所有数据都以字节数组存储，将大大压缩对象的大小，可能达到成倍的效果，其实也就是没有对象。若堆外内存充裕的时候，实现这个特性并不会导致内存爆掉，除非几十G中的数据只有一个key这种特殊情况。我们想实现堆外groupByKey的mapSideCombine一个重要原因就是，我考虑在map端也聚合的话可以减轻read端的操作和内存压力，否则reduceByKey也不会有这个特性，当然这只是猜想，性能是否有提升还是要实验数据说话。 具体的实现首先是record的插入：12345678910111213141516171819202122232425262728293031323334public void insertOrUpdateGroupRecord( Object originKeyBaseObject, Object keyBaseObject, long keyBaseOffset, int keyLengthInBytes, int partitionId, Object valueBaseObject, int valueBaseOffset) throws IOException &#123; if (isContain(keyBaseObject, keyBaseOffset, keyLengthInBytes, addressLength)) &#123; //compact buffer数组缓冲区 //key已经存在 long oldValueAddress = PlatformDependent.UNSAFE.getLong(loc.getPage(), loc.getPosition() + 4 + keyLengthInBytes); long newValueAddress = oldValueAddress; switch(valueType)&#123; case 0: newValueAddress = UnsafeBuffer.putInt(null,(Long)oldValueAddress,valueType,(Integer)valueBaseObject); break; case 1: newValueAddress = UnsafeBuffer.putLong(null,(Long)oldValueAddress,valueType,(Long)valueBaseObject); break; case 2: newValueAddress = UnsafeBuffer.putDouble(null,(Long)oldValueAddress,valueType,(Double)valueBaseObject); break; default : assert(valueType &lt; 2); break; &#125; //更新地址值 PlatformDependent.UNSAFE.putLong(loc.getPage(),loc.getPosition() + 4 + keyLengthInBytes,newValueAddress); &#125;else&#123; 这里是key已经存在的操作，isContain类似于hashmap的contain方法，查找key是否存在，若存在key，获取到value，也就是对应key的那一组value的地址，而UnsafeBuffer就是实现那一组value存储区域的类，它负责区域的扩容与内存申请。插入记录可能会导致valueAddress的变化，所以每一次都要更新。 接着write方法会调用closeAndWriteOutput()这个方法，用于将排好序的KV写入磁盘文件中，用于下一个stage shuffle的读取，跟踪进去需要改写一下writeSortedFile这个方法：12345678910111213141516171819202122232425final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();final Object recordPage = memoryManager.getPage(recordPointer);final long recordOffsetInPage = memoryManager.getOffsetInPage(recordPointer);int dataRemaining = PlatformDependent.UNSAFE.getInt(recordPage, recordOffsetInPage);long recordReadPosition = recordOffsetInPage + 4; // skip over record lengthint dataSize =dataRemaining;//key+value/addressif(!needGroup) &#123; while (dataRemaining &gt; 0) &#123; // logger.info("dataRemaining:"+dataRemaining); //assert(dataRemaining == 8l); final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining); PlatformDependent.copyMemory( recordPage, recordReadPosition, writeBuffer, PlatformDependent.BYTE_ARRAY_OFFSET, toTransfer); writer.write(writeBuffer, 0, toTransfer); recordReadPosition += toTransfer; dataRemaining -= toTransfer; &#125;&#125; 原本只是往文件中一个一个的KV写入，但现在的V指的是一组value的堆外起始地址，根据这个valueAddress才能找到真实的value值，我们现在要先写入key，接着是value的总大小，后面跟对应该key的所有value，这时候就不需要写入valueAddress的地址了，因为read端不需要这个变量。也就是加上这一段代码：123456789101112131415161718192021222324252627282930313233343536373839404142//不用写address dataRemaining-=8; while (dataRemaining &gt; 0) &#123; // logger.info("dataRemaining:"+dataRemaining); //assert(dataRemaining == 8l); final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, dataRemaining); PlatformDependent.copyMemory( recordPage, recordReadPosition, writeBuffer, PlatformDependent.BYTE_ARRAY_OFFSET, toTransfer); writer.write(writeBuffer, 0, toTransfer); recordReadPosition += toTransfer; dataRemaining -= toTransfer; &#125; // /** // by kzx //暂定int型key so 为4 int keySize = dataSize - 8; long valueAddress = PlatformDependent.UNSAFE.getLong(recordPage, recordOffsetInPage + 4 + keySize); //skip第一个size long valuePosition = valueAddress + 4 ; int valueSize = PlatformDependent.UNSAFE.getInt(null, valuePosition)-4;//减去第一个size //结构为size,realValue... int valueRemaining = valueSize; while (valueRemaining &gt; 0) &#123; final int toTransfer = Math.min(DISK_WRITE_BUFFER_SIZE, valueRemaining); PlatformDependent.copyMemory( null, valuePosition, writeBuffer, PlatformDependent.BYTE_ARRAY_OFFSET, toTransfer); writer.write(writeBuffer, 0, toTransfer); valuePosition += toTransfer; valueRemaining -= toTransfer; &#125; //记住这里写进去的valueSize没有减4 //add by kzx,free memory PlatformDependent.UNSAFE.freeMemory(valueAddress); &#125; 切记不要忘了在写完一条记录之后要释放掉对应的堆外内存！ 然后就是read端的改写，这里花了有点时间。众所周知，Spark是惰性执行，代码中充满着各种各样的迭代器，追踪代码时都不知道哪个迭代器被调用了。最后调试发现调用路径是这样的FlintHashShuffleReader-&gt; BlockStoreShuffleFetcher-&gt; ShuffleBlockFetcherIterator.这个iterator里面的next会有一个获取Key-Value对的地方，其实也就是做对应的反序列化，我要改的地方也就在读取反序列化读取Value这儿。12345678910111213141516171819/** Reads the object representing the value of a key-value pair. */override def readValue[T: ClassTag](): T = &#123; val redundantSize = 4+4 //包含了buffer中的两个size if (multiValue) &#123; val valueSize = ldis.readInt() if(valueType == 0)&#123; //int val valueNum = (valueSize-redundantSize)/4 val valueArray = new Array[Integer](valueNum) var index = 0 while(index &lt; valueNum)&#123; val temp = ldis.readInt() valueArray(index) = temp index+=1 &#125; valueArray.asInstanceOf[T] &#125;else if(valueType == 1)&#123; ... &#125; 从读单个value改写成读一组value。这里的size有点绕，起先我遇到的bug就是这个点造成的。 测试事情往往总是事与愿违的，也是出乎意料的。优化的效果并不好，做了与加这个特性之前的Deca比较实验：read端的确快了一点，原因有二： 1.减少了大量的key的查询，因为很多相同key的value已经在value端聚合在一起了。 2.shuffle文件比原有的size要小，减少了网络的传输。 不过效果算得上微乎其微。最要命的是，write端由于多余的聚合操作，运行时间从2.8min提高到约4min。。平心而论，算得上失败的一次尝试了。 后续对于优化效果我是存有疑问的，read端不至于优化效果如此微小，这方面我暂时不想达到微小的工作这个水平。//12月5日更新后来我在代码中加了一些log，发现shuffle的map端内存中聚合是写文件花费时间的好几倍，这里的原因是原数据文件中相同key的value并不多，我查了一下大概就四十多个，也就是不存在hot key。所以shuffle文件的大小在加不加mapsideCombine的两种情况下相差并不大，所以当key的数量很少时，有mapSideCombine的groupByKey性能才回更有优势，insertSorter和write之间需要达到一种平衡。这个我正在做测试。不过也给了一种启发，如果每个partition相同的key不多，而且每个key存在大量value时，采用mapsideCombine的groupBykey是一个不错的选择。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[偶遇超级僵尸进程]]></title>
      <url>%2F2016%2F11%2F28%2Fsuper-zombie%2F</url>
      <content type="text"><![CDATA[你怎么死来死去都死不了啊？ 其实我差点就死了，你再给我多一点点时间，我就死定了。 ----喜剧之王 写在前面的话僵尸进程是经常遇到的，解决起来也比较方便。但一个偶然的机会遇到了一个父进程为1号进程的的僵尸进程，这个父进程肯定是不能随意杀死的，否则会导致严重的后果，至于什么严重的后果我也没测试过，搜了一下好像也没人细说。所以我私自地给它命名为超级僵尸进程，听起来给人一种很厉害的感觉，但又略带有一些调侃，加上上面的一句喜剧之王的台词，显得饶有趣味，好吧，岔开了，进入正题。 Normal Zombie关于普通僵尸进程首先它是如何产生的？一个进程终止的方法很多，进程终止后有些信息对于父进程和内核还是很有用的，例如进程的ID号、进程的退出状态、进程运行的CPU时间等。因此进程在终止时回收所有内核分配给它的内存、关闭它打开的所有文件等等，但是还会保留以上极少的信息，以供父进程使用。父进程可以使用 wait/waitpid 等系统调用来为子进程收拾，做一些收尾工作。因此，一个僵尸进程产生的过程是：父进程调用fork创建子进程后，子进程运行直至其终止，它立即从内存中移除，但进程描述符仍然保留在内存中（进程描述符占有极少的内存空间）。子进程的状态变成EXIT_ZOMBIE，并且向父进程发送SIGCHLD信号，父进程此时应该调用wait()系统调用来获取子进程的退出状态以及其它的信息。在 wait 调用之后，僵尸进程就完全从内存中移除。因此一个僵尸存在于其终止到父进程调用wait等函数这个时间的间隙，一般很快就消失，但如果编程不合理，父进程从不调用 wait 等系统调用来收集僵尸进程，那么这些进程会一直存在内存中.概述来说就是两点原因：1.子进程终止后向父进程发出SIGCHLD信号，父进程默认忽略了它；2.父进程没有调用wait()或waitpid()函数来等待子进程的结束。如何发现系统中的僵尸进程？可以运行命令–&gt;ps -aux |grep -w ‘Z’ (这里的-w参数表示精确匹配) 如何杀掉普通僵尸进程把父进程杀掉，僵尸进程会变成孤儿进程，然后过继给1号进程，而1号进程会扫描名下子进程，把 Z 状态进程回收；这时候僵尸进程已经退出了，只保留了task_struct结构体，所以发信号（-9等信号）去处理僵尸进程是无效的； Super Zombie超级僵尸进程通过对普通僵尸进程的分析,这样看起来好像父进程为1号进程的进程不会成为僵尸进程了，因为1号进程都会时刻扫描其子进程的状态，发现是僵尸进程就会马上去释放它的资源。但是，父进程为1号进程的进程 其实也是有可能成为僵尸进程的。下面说几种情况：1、进程还在被其它进程使用，退出；2、进程的子线程还在执行任务，但主线程已经死掉了（可能主线程已经被杀了，systemd停止服务时会发SIGTERM信号）；3、进程阻塞在某一IO请求上,这时控制权已交到内核手上,这时如果子进程被KILL掉,那么就成为父进程ID为1的僵尸进程,这个进程不会退出,会一直阻塞直到IO请求被满足。 应对超级僵尸进程其实ppid=1的僵尸进程可以不用去管他,因为它迟早会被1号进程回收的如果有很多僵尸进程除外.并且绝大部分ppid=1的僵尸进程是暂时的：1、当进程被跟踪调试完，则会自动被回收掉的；2、其他子线程组的线程执行完后会自动退出，僵尸进程会被回收；3、这个可能会一直挂着，如果阻塞的io永远没有到达； 总之，遇到少量的僵尸进程，可以不需要特意的去处理，只需要查看下根源，看看是否有潜在的bug就可以；]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论如何调配jvm]]></title>
      <url>%2F2016%2F11%2F26%2Fjava-optimize%2F</url>
      <content type="text"><![CDATA[一些要交代的我的研究生主题大概可以用两个字概括，那就是搞spark。两个字可能不太严谨，因为读出来感觉就是搞斯帕克。其历经之路也可以高度总结一下: 安装配置spark -&gt; 写常用的spark应用 -&gt; 读spark源码 -&gt; 优化spark。当然优化spark，并不一定要从内核入手，有很多种方案，比如写出优雅的应用程序，调配置参数，官方还给出了一个重要的优化方案，那就是jvm优化，毕竟spark这种分布式系统很多都跑在jvm上。附上spark团队给的优化向导地址，里面有一部分讲的就是垃圾回收策略的选择与调控。 好的，让我们来快速聊下jvm^_^jvm的构成: jvm区域总体分两类，heap区和非heap区。heap区又分：Eden Space（伊甸园）、Survivor Space(幸存者区)、TenuredGen（老年代-养老区）。非heap区又分：Code Cache(代码缓存区)、Perm Gen（永久代）、Jvm Stack(java虚拟机栈)、Local Method Statck(本地方法栈)。曾经看过一个举例很形象，大致解释了下jvm对象的生与死。假设你是一个普通的Java对象，你出生在Eden区，在Eden区有许多和你差不多的小兄弟、小姐妹，可以把Eden区当成幼儿园,在这个幼儿园里大家玩了很长时间。Eden区不能无休止地放你们在里面，所以当年纪稍大，你就要被送到学校去上学，这里假设从小学到高中都称为 Survivor区。开始的时候你在Survivor区里面划分出来的的“From”区，读到高年级了，就进了Survivor区的“To”区，中间由于学习成绩不稳定，还经常来回折腾。(这里指发生了18次young GC)直到18岁的时候，高中毕业了，该去社会上闯闯了。于是你就去了年老代，年老代里面人也很多。在年老代里,你生活了20年每次GC加一岁),最后寿终正寝，被GC回收。有一点没有提，你在年老代遇到了一个同学，他的名字叫海绵宝宝，他以及他的海绵家族永远不会死，那么他们就生活在永生代。这里解释下永生代，这里面放着jvm不会去主动回收的东西，如类模版对象，方法模版对象，还有在JDK7之前的HotSpot虚拟机中，字符串常量池的字符串被存储在永久代中，因此导致了一系列的性能问题和内存溢出错误。所以在java8中移除了永生代，迎来了元空间。 让年轻代大一点较大的年轻代由于Full GC的成本远远高于MinorGC，因此某些情况下需要尽可能将对象分配在年轻代，这在很多情况下是一个明智的选择。虽然在大部分情况下，JVM会先尝试在Eden区分配对象，但是由于空间紧张等问题，很可能不得不将部分年轻对象提前向年老代压缩。因此，在JVM参数调优时可以为应用程序分配一个合理的年轻代空间，以最大限度避免新对象直接进入年老代的情况发生。而且如果JVM参数可以设置-XX:+PrintGCDetails -Xmx20M -Xms20M，这时候指jvm堆空间设置的初始内存为20M，最大也为20M。个人认为大部分情况下(比如你大概可以预估到你的程序所占内存有多大)初始堆内存和最大堆内存设置得相同较为合理一点，这样减少了jvm堆空间拓展的开销，因为如果虚拟机启动时设置使用的内存比较小，这个时候又需要初始化很多对象，虚拟机就必须重复地增加内存。分配足够大的年轻代空间，使用JVM参数-XX:+PrintGCDetails -Xmx20M -Xms20M-Xmn6M,年轻代大小为6M。 较大较合理的幸存者区使用率在jvm年轻代中也可以做一些配置比例上的文章，比如，设置合理的Survivor区并且提供Survivor区的使用率，可以将年轻对象保存在年轻代。一般来说,Survivor区的空间不够，或者占用量达到50%时，就会使对象进入年老代，不管它的年龄有多大.SurvivorRatio也可以优化survivor的大小,不过这对于性能的影响不是很大。SurvivorRatio是eden和survior大小比例。参数是-XX:SurvivorRatio=8。 让大对象进入老年代进入老年代你需要多大size我们在大部分情况下都会选择将对象分配在年轻代。但是，对于占用内存较多的大对象而言，这就会落入比较尴尬的境地。因为大对象出现在年轻代很可能扰乱年轻代GC，并破坏年轻代原有的对象结构。因为尝试在年轻代分配大对象，很可能导致空间不足，为了有足够的空间容纳大对象,JVM不得不将年轻代中的年轻对象挪到年老代。因为大对象占用空间多，所以可能需要移动大量小的年轻对象进入年老代,这对GC相当不利。基于以上原因，可以将大对象直接分配到年老代，保持年轻代对象结构的完整性，这样可以提高GC的效率。如果一个大对象同时又是一个短命的对象，假设这种情况出现很频繁，那对于GC来说会是一场灾难。原本应该用于存放永久对象的年老代，被短命的对象塞满，这也意味着对堆空间进行了洗牌，扰乱了分代内存回收的基本思路。因此，在软件开发过程中，应该尽可能避免使用短命的大对象，这是开发者应该走到的。可以使用参数-XX:PetenureSizeThreshold设置大对象直接进入年老代的阈值。当对象的大小超过这个值时，将直接在年老代分配。如果需要将1MB以上的对象直接在年老代分配，设置-XX:PetenureSizeThreshold=1000000，这里的单位是字节。 进入老年代你需要多老堆中的每一个对象都有自己的年龄。一般情况下，年轻对象存放在年轻代，年老对象存放在年老代。为了做到这点，虚拟机为每个对象都维护一个年龄。如果对象在Eden区，经过一次GC后依然存活，则被移动到Survivor区中，对象年龄加1。以后，如果对象每经过一次GC依然存活，则年龄再加1。当对象年龄达到阈值时，就移入年老代，成为老年对象。这个阈值的最大值可以通过参数-XX:MaxTenuringThreshold来设置，默认值是15。虽然-XX:MaxTenuringThreshold的值可能是15或者更大，但这不意味着新对象非要达到这个年龄才能进入年老代。事实上，对象实际进入年老代的年龄是虚拟机在运行时根据内存使用情况动态计算的(比如说年轻代空间不够了，需要将很多对象提前移入老年代)，这个参数指定的是阈值年龄的最大值。即，实际晋升年老代年龄等于动态计算所得的年龄与-XX:MaxTenuringThreshold中较小的那个。 设置-XX:MaxTenuringThreshold=1指年轻对象经过一次GC后便进入老年代. 其他的一些技巧降低GC时的STWSTW即stop the world，垃圾回收时会暂停所有应用线程来用于垃圾回收,首先考虑的是使用关注系统停顿的CMS回收器,因为并发清理这个阶段收集器线程和应用程序线程会并发执行,–XX:+UseConcMarkSweepGC年老代使用CMS收集器降低停顿； 减少full GC应尽可能将对象预留在年轻代，因为年轻代Young GC的成本远远小于年老代的Full GC。–XX:ParallelGCThreads=20设置 20 个线程进行垃圾回收；–XX:+UseParNewGC年轻代使用并行回收器；–XX:+SurvivorRatio：设置Eden区和Survivor区的比例为 8:1。稍大的Survivor空间可以提高在年轻代回收生命周期较短的对象的可能性，如果Survivor不够大，一些短命的对象可能直接进入年老代，这对系统来说是不利的。–XX:MaxTenuringThreshold设置年轻对象晋升到年老代的年龄。默认值是15次，即对象经过15次Minor GC依然存活，则进入年老代。这里设置为31，目的是让对象尽可能地保存在年轻代区域。 最后附上一个介绍jvm参数配置大全的文章：文章地址]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[科学配置hive+mysql+hadoop]]></title>
      <url>%2F2016%2F11%2F24%2Fhive-mysql%2F</url>
      <content type="text"><![CDATA[mysql配置先前的服务器mysql有一些莫名地故障，于是准备重装。贸然重装肯定是一件愚蠢的事，首先要做的是把原有的mysql卸载干净。但如果你的mysql能正常地工作，那当我没提起这件事。卸载遵循以下步骤： *运行rpm -qa|grep -i mysql。 显示如下： *删掉它们，当然之前要停止mysql服务，运行rpm -e --nodeps 包名 *查找之前老版本mysql的目录、并且删除老版本mysql的文件和库,运行find / -name mysql，会出现很多关于mysql的目录，删掉它们。 *run "rm -rf /etc/my.cnf" *再次查找机器是否安装mysql --&gt;rpm -qa|grep -i mysql 接下来安装mysql，方便起见，采用rpm的安装方式。下载mysql全家桶下载地址，然后解压，安装=》sudo rpm -ivh MySQL-*。顺便转一下配置文件=》sudo cp /usr/share/mysql/my-large.cnf /etc/my.cnf。 开启mysql服务=》 service mysql start。 设置用户密码=》 sudo /usr/bin/mysqladmin -u root password ‘123’ 进入mysql=》mysql -uroot -p123 创建mysql的hive用户=》 CREATE USER ‘hive’@’localhost’ IDENTIFIED BY “123” 创建数据库=》create database hive; 赋予权限=》 grant all on hive. to hive@’%’ identified by ‘hive’; grant all on hive. to hive@’localhost’ identified by ‘hive’; flush privileges; 退出mysql顺便可以验证下hive用户=》mysql -uhive -phive hadoop安装略不过注意一个问题：之前hadoop start起来没问题，也没有仔细看日志，但是运行hadoop命令总是遇到connection refused的问题，之前以为防火墙的原因。后来发现是用户bashrc的环境配置默认hadoop是另一个版本的，所以hadoop的命令工具来自另个hadoop。这个错误原因有点离谱，但还是挺难发现的。 hive安装与配置安装配置首先下载 hive1.2.1=》 nohup wget http://mirrors.hust.edu.cn/apache/hive/stable/apache-hive-1.2.1-bin.tar.gz &amp;解压 tar -zxvf ..配置环境变量 .bashrc=&gt;export HIVE_HOME=/home/feiwang/hive-2.0export HIVE_CONF_DIR=/home/feiwang/hive-2.0/confexport PATH=$PATH:$HIVE_HOME/binsource .bashrc修改hive配置文件，将默认文件mv成hive-site.xml，不是hive-default.xml！之前一直读取配置有问题就是这个原因。主要修改以下参数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL &lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive &lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName &lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver &lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword &lt;/name&gt; &lt;value&gt;hive &lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.listen.port &lt;/name&gt; &lt;value&gt;9999 &lt;/value&gt; &lt;description&gt;This is the port the Hive Web Interface will listen on &lt;/descript ion&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://127.0.0.1:9083&lt;/value&gt; &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: $&#123;hive.exec.scratchdir&#125;/&amp;lt;username&amp;gt; is created, with $&#123;hive.scratch.dir.permission&#125;.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive/local&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp/hive/resources&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;&lt;/property&gt; 下载jdbc的jar包=》wget http://115.156.188.231/cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.40.tar.gz 拷贝mysql-connector-java-5.1.40-bin.jar 到hive 的lib下面mv mysql-connector-java-5.1.40-bin.jar /root/feiwang/hive-2.0/lib/ 启动元数据 =》bin/hive –service metastore &amp; 验证运行 hive命令,创建表：进入mysql会发现hive数据库自动生成了很多表进入hadoop的hdfs会发现也有两个文件夹生成了Found 2 itemsdrwxr-xr-x - root supergroup 0 2016-11-24 21:43 /user/hive/warehouse/diablodrwxr-xr-x - root supergroup 0 2016-11-24 19:45 /user/hive/warehouse/kzx 简单介绍下hiveHive是运行在Hadoop之上的数据仓库，将结构化的数据文件映射为一张数据库表，提供简单类SQL查询语言，称为HQL，并将SQL语句转换成MapReduce任务运。有利于利用SQL语言查询、分析数据，适于处理不频繁变动的数据。Hive底层可以是HBase或者HDFS存储的文件。两者都是基于Hadoop上不同的技术，相互结合使用，可处理企业中不同类型的业务，利用Hive处理非结构化离线分析统计，利用HBase处理在线查询。Hive三种元数据存储方式：1&gt;.本地derby存储，只允许一个用户连接Hive，适用于测试环境2&gt;.本地/远程MySQL存储，支持多用户连接Hive，适用于生产环境]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在大菠萝中遇到的一些小问题]]></title>
      <url>%2F2016%2F10%2F10%2Fdiablo%2F</url>
      <content type="text"><![CDATA[大菠萝的任务大菠萝全名diablo technology,是一家硬件技术外企.但是我是一名纯软界的小小码农,怎么会帮一个硬件公司做事呢.起初是这家公司中国区的技术人员跟我们组里的陆博大大比较熟,他们想招一两个懂Spark的人帮他们做一些测试和性能上的优化来得出他们研发的新硬件M1相比于市场上普通内存的特性与性能差异.于是陆博建议我担任这份兼职,工作不多而且还可以增一份不错的收入,众所周知,这种事情我是无法拒绝的. 为了推广M1,他们需要在M1上运行很多工业市场上流行的吃内存的软件与平台,包括spark,mysql,redis等.所以这三个月我就没事帮他们写一点测试的代码,优化spark的配置,寻找运行错误的bug,搭建redis-cluster这种活.期间爬过很多坑,有几个瞬间我感觉到自己是技术顾问,让我小小地膨胀一下. C程序的测试C语言我是有好几年没碰了,作为一个大四以后一直在jvm语言中游走,有时用python写点小项目的人,几乎对指针这种东西处于懵逼模式. 代码的编写和测试流程我就不详细说了,说说遇到的段错误解决方案.也是这次跑C程序让我有了了解这方面知识的契机.发生了段错误/core dump,总结一下原因,一般是这几个: 访问了不存在的内存地址 访问了系统保护的内存地址 访问了只读的内存地址等等情况 先用gdb -g -o xx xx.c 生成gdb可调试的文件。说到gdb调试,大概要知道这几个命令: gdb xx 进入gdb调试界面 b 行数 在第多少行打断点 按r进入运行状态 n是单步调试 s进入函数中调试 q是退出 继续说解决core dump的bug, 然后运行程序, ./xx发生段错误后会生成core文件.接着运行命令gdb xx core.36129.输入where 会打印出具体的core dump发生在代码中哪个位置，方便定位bug。 还有一个bug是,每次都在内存地址被free的时候报无效指针,这种情况一般包含以下几个原因: 一个地址被free了两次, 当然这个很容易查出来. free的地址已经不是当时申请内存的起始地址了,注意可能在程序中的一些函数中对其做了一些操作. Spark-Sql测试框架中踩得坑大菠萝公司的喵叽(算我们的上司大人)想在机器上测试Spark Sql在M1上跑的性能，让我调研下Spark-sql-perf这个已有的测试框架(项目地址)。看了一下框架代码，差不多就是写了很多sql语句封装成一个个benchmark对象然后有一个多线程模式，然后一起开测。首先，在大菠萝的机器上部署测试框架，然后写一个简单的测试程序。代码如下:1234567891011121314151617181920212223object SqlTest extends Serializable&#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName("test") .setMaster("local") val sc = new SparkContext(sparkConf) val sqlContext = new org.apache.spark.sql.SQLContext(sc) val tables = new Tables(sqlContext, "/Users/iceke/projects/tpcds-kit/tools", Integer.parseInt("1")) tables.genData("data","parquet",true, false, false, false, false) val tableNames = Array("call_center", "catalog_page", "catalog_returns", "catalog_sales", "customer", "customer_address", "customer_demographics", "date_dim", "household_demographics", "income_band", "inventory", "item", "promotion", "reason", "ship_mode", "store", "store_returns", "store_sales", "time_dim", "warehouse", "web_page", "web_returns", "web_sales", "web_site") for(i &lt;- 0 to tableNames.length - 1) &#123; val a = sqlContext.read.parquet("data" + "/" + tableNames&#123;i&#125;) // sc.broadcast(a) a.registerTempTable(tableNames&#123;i&#125;) &#125; val tpcds = new TPCDS (sqlContext = sqlContext) val experiment = tpcds.runExperiment(tpcds.tpcds1_4Queries, iterations = 1,forkThread=false) experiment.waitForFinish(60*60*10) &#125; 先生成数据，大概几十张表。然后加载这几十张表，进行benchmark实验。本来过程很简单，但是需求是无止境的，喵叽说试试把所有的表cache到内存中，这样进行查询时快一点，听上去非常简单，在注册后加上一行cache表的代码就可以了。部分代码如下：1234 val a = sqlContext.read.parquet("data" + "/" + tableNames&#123;i&#125;)// sc.broadcast(a) a.registerTempTable(tableNames&#123;i&#125;) sqlContext.cacheTable(tableNames&#123;i&#125;) 我在mac上用local模式测了一下ok便提交给喵叽。 事情永远不会像想象的那么顺利，喵叽在集群上一测，就通过了几个benchmark，然后大量的stage报错，全部都是failed to get broadcast(TorrentBroadcast)异常，然后stage 直接失败，查看executor的日志发现已经有的broacast被remove了，但是接下来的加个task又会去获取这些broadcast，便会直接失败。这明显是不符合逻辑的，明明cache了所有表，靠异常堆栈信息并不好定位到错误的地方。我看了spark主页发现storage页面cache的RDD过一段时间就会消失，这也是一个重要的线索。 测试了半天还是出错，我开始静下心慢慢跟着框架的代码走，我发现最后每一个线程都会执行一个doBenchmark方法，方法里面最后执行完查询操作之后会做一些善后处理。相关代码：1234567891011121314151617final def benchmark( includeBreakdown: Boolean, description: String = "", messages: ArrayBuffer[String], timeout: Long, forkThread: Boolean = true): BenchmarkResult = &#123; logger.info(s"$this: benchmark") sparkContext.setJobDescription(s"Execution: $name, $description") beforeBenchmark() val result = if (forkThread) &#123; runBenchmarkForked(includeBreakdown, description, messages, timeout) &#125; else &#123; doBenchmark(includeBreakdown, description, messages) &#125; println("!!!!!!!!!!!!!!!!after Bench") afterBenchmark(sqlContext.sparkContext) result 这个善后处理是afterBenchmark方法，方法代码：12345678private def afterBenchmark(sc: SparkContext): Unit = &#123; // Best-effort clean up of weakly referenced RDDs, shuffles, and broadcasts System.gc() // Remove any leftover blocks that still exist sc.getExecutorStorageStatus .flatMap &#123; status =&gt; status.blocks.map &#123; case (bid, _) =&gt; bid &#125; &#125; .foreach &#123; bid =&gt; SparkEnv.get.blockManager.master.removeBlock(bid) &#125;&#125; 它会对每个相关block的id进行移除，无论它是否做了cache。所以解决方法是注释掉afterBenchmark方法，与上次堆外内存的bug一样，发现bug的过程是痛苦的，解决方案是简单到发指的,让人痛苦,又让人快乐. 所以说，别人的框架不是万能的，虽然你操作起来更便捷，但你必须遵守它制定的那一些规则，有些规则是操蛋的，当你使用它时，如果一直被操蛋的bug所围困，就需要看看它的源码看看是否有问题，或者与你的策略存在偶然性的冲突。 最后, 我的梦想是成为规则的制定者.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[修改Spark内核(Deca)所产生的两个奇怪bug]]></title>
      <url>%2F2016%2F09%2F28%2Fspark-bug%2F</url>
      <content type="text"><![CDATA[Spark将数据放在jvm堆外导致无故卡死思路实现与问题产生咱们内存计算小组陆博的文章Deca经过一年的努力终于中了vldb,作为其中参与了一些微小的工作的男人,虽然对其中一部分细节不是太清楚,因为这个Deca系统的开发涉及到将近5个人,我的工作囊括一下就是: 1.完成了其中UDF的转换,将方法的操作转为对字节数组的操作; 2.手写了Deca的手动版的代码,就是利用Deca的思想对Spark应用的代码进行改造; 3.进行了大量的测试并统计GC时间,stage时间等相关数据. 其实Deca系统的核心思想就是将原有的java大对象转化为字节数组有序地放置在jvm中,这样一可以减少对内存的使用,也可以基本避免所有的GC.附上论文Deca论文地址 老板的top中了之后,当然会将它扩展扩展然后投期刊,这是基本套路.要求扩展30%,其中就包括将之前的手动版的数据放置在堆外,还是以字节数组的形式来和堆内版本进行比较,理论上来说堆外版本肯定性能是比堆内好的,毕竟放置在堆外可以完全逃避GC的控制,也更加符合Deca的思想. 代码实现并不难,基本由Unsafe这个类操作完成.大概思路就是将Spark应用中需要缓存的RDD其中的partition的对象用字节数组的形式写在堆外,读的时候再直接按照偏移量读取,贴上部分代码:1234567891011import UnsafePR._private val baseAddress = UNSAFE.allocateMemory(size)private var curAddress = baseAddressdef address = baseAddressdef free:Unit=&#123; UNSAFE.freeMemory(baseAddress)&#125;def writeInt(num:Int):Unit=&#123; UNSAFE.putInt(curAddress,num) curAddress += 4&#125; 完整的Deca手动版PageRank代码地址 起先在本机的local模式测试了堆外版LR和PageRank应用当然是没问题的,结果也是正确的.然后转移到服务器集群上进行测试.令人惊喜的是,一个神奇的bug出现了. Bug的特征此Bug是本人coding以来见识到的算的上奇怪的一个bug了,它有以下几个特征.首先LR的堆外版本集群测试是没有问题的,但是切换到PageRank的堆外版本来测试时,总是在ZipPartition这个stage最后几个task执行的时候jvm crash掉,这个job就直接卡死了,必须手动杀掉才能停止.executor异常日志: stage卡住图示: 而且还有一个特殊的症状:就是PR跑2G数据量的时候居然不会挂掉,一到7G和20G的时候就会挂掉.而且local模式不会出错,一个executor也不会出错,一旦增加到多个executor就会出错. Bug原因分析与结论一开始想到的原因是shuffle的问题,因为local和单个executor不会出错,一旦涉及到网络传输就会报错.我怀疑是不是序列化方式的问题,分别用kryo和java自带的序列化方式测试了一下,然而都会报错.后来想了想应该不是网络传输的问题,不然小数据量怎么可以通过.我上网查了一下jvm crash那段报错信息,基本都是由于Unsafe访问到非法位置的原因,于是开始往这个方向考虑. 最后与师兄讨论中意识到问题的关键所在,首先Spark中一个task在相应的节点没有可空闲的资源来启动task,如果等待一定时间(可配置)还是没有资源则调度到其他节点去执行,也就是non-local task.non-local task是从网络传输过去的,这个策略也就是延迟调度策略.这部分task是由cache RDD的partition生成而来的,这部分task是从block manager过去的,然而partition中的UnsafeEdge对象中只有一个Unsafe成员变量,一个初始地址和终点地址,和分配在jvm堆内的对象不同,并不携带真正的数据.所以这个task被调度到其他executor时,自然会非法访问堆外内存,然后jvm crash掉,这也可以解释为什么stage中位置为Any的task都不能成功执行这个现象.至于2G的数据量为什么可以通过,因为task运行的时间很短,几乎不需要调度就可以在一个executor中全部完成. 所以解决方案就是尽量不让task调度到其他的executor上执行,可以尽量增大spark.locality.wait这个变量来避免出错. 硕大的cache数据这个就简洁地介绍一下了,这是我大概11月10号遇见的 背景关于shuffle的VST拆解部分需要加到论文修改中,不过在意外中我发现了之前Deca release1.0版本的一个bug.那就是由于Deca将cache数据的对象完全转化为字节数组存储在jvm中,但是吊诡的现象在于Deca cache的数据居然比原生Spark的还大,这明显是不合理的.初看了一下代码发现是没有问题的.于是联系已经毕业的裴师兄,他说他之前就遇到这个问题了,只不过一直没改.这应该就是传说中的前人挖坑,后人填坑.但我同时也是很兴奋的,因为我们之前手动版本的实验结果很合理,改动Spark内核的自动版本也是同样地思路和流程,但是却出现了这种奇怪的现象,你知道解决bug是一个很能产生成就感的一个举措.在SparkContext将cache的RDD的迭代器做了一次调整,生成一个新的RDD并cache,然后将原本cache的RDD释放掉,重新调整一下RDD链,这样缓存的RDD将会被我们生成的RDD替换掉.之前对缓存的RDD所做的操作是:将里面返回KV对的迭代器改写一下,变换成往一个字节数组缓冲区写字节数组(按顺序写),然后返回新的迭代器. 修复后来发现之前返回的迭代器基本单位还是一个个KV对,这样就算是按字节数组写了也还是和Spark Cache的数据占用着差不多的大小,于是很简单啦,这里我将一块字节数组区称为CacheChunk,事实上它的类名也是这个.生成CacheChunk后,用Iterator包装一下便返回.跑了一下local模式,结果可想而知,抛出血红色的异常,我用的Idea,不知道其他的编辑器是不是这样.众所周知,Spark的每一个stage的结束要么是ShuffleTask,要么是ResultTask.ShuffleTask需要落磁盘,往block写点什么,这时候出发真正的RDD计算,就是调用RDD的迭代器.当然这里有个判断,如果某个RDD被定义了persist,第一次计算时会将它的计算结果常驻在内存中再返回迭代器,这样下一个stage用到此RDD时便直接在内存中获取,无需计算.管理这个流程的是一个叫CacheManager的哥们,我们之前返回的迭代器是一个iterator,iterator里包含着CacheChunk,CacheChunk里面又有一个迭代器方法,所以CacheManager肯定识别不了了呗.方便,给这个变换的CacheRDD加一个标签变量,切勿不要给它trasient这个标识,它需要被序列化.如果是Deca的RDD便让CacheManager多一个步骤,获取迭代器之后,强制转换为CacheChunk,再取一次迭代器,这样就能获取真正的数据.好,测试通过,perfect.自信地放到集群中去测试,令人”欣慰”的是,cache的数据反而更巨大! 后来怎么办呢,研究CacheManager下面的代码,继续调试往里面跟着走,我只想感叹一句,真的很深…就连一个普通的HashMap Spark都会根据自己的需求改.最后发现RDD的大小的评估是一个SizeEstimator的类实现的,部分功能代码如下: 12345678910111213141516private def visitSingleObject(obj: AnyRef, state: SearchState) &#123; val cls = obj.getClass if (cls.isArray) &#123; visitArray(obj, cls, state) &#125; else if (obj.isInstanceOf[ClassLoader] || obj.isInstanceOf[Class[_]]) &#123; // Hadoop JobConfs created in the interpreter have a ClassLoader, which greatly confuses // the size estimator since it references the whole REPL. Do nothing in this case. In // general all ClassLoaders and Classes will be shared between objects anyway. &#125; else &#123; val classInfo = getClassInfo(cls) state.size += alignSize(classInfo.shellSize) for (field &lt;- classInfo.pointerFields) &#123; state.enqueue(field.get(obj)) &#125; &#125;&#125; 大概思路就是判断对象类型,原生类型直接算,数组就累加算.如果不是原生类型,继续将里面的成员变量入栈,递归调用此函数.当我调试到这儿看调试信息的时候,发现一共访问了有一千多个成员变量,明显不合理.后来发现,CacheChunk里有个Spark定制的IterupptedIterator,这里面带出一批Spark相关的变量,导致评估大小大了整整十几倍.可是CacheChunk是可以不需要这个iterator的,于是将它换了个地方.重新测试,终于okay.流下了喜悦的眼泪,想高歌一曲,想起在实验室便作罢. 之后还遇到CacheChunk自动扩增容量的问题,不过解决起来比较简单,就不在此描述.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论思想同化的艺术]]></title>
      <url>%2F2016%2F09%2F11%2Fassimilation%2F</url>
      <content type="text"><![CDATA[看过一段访谈,一个摇滚乐队主唱的访谈,这个人叫邓斐,乐队名字叫寂寞.夏.日.曾经有一段时间疯狂地听他们的歌.他谈到他们的歌《这个易同化的民族》时说道: "我们正在被对权力的卑躬屈膝同化,正在被对文化和艺术弱智的认知同化,我们拿来从不消化甚至不知何物就充当自己, 我们做事不认真不努力随后从不自我反省反而学会相互制约和压制欺骗,我们正在被机会主义同化从无察觉..." 他想批判的很多,在其歌词中也可见一斑,他在我眼中算得上摇滚界的当代杜甫.这里要说到摇滚是什么,窦唯曾经说过:对于摇滚没有必要给它做个定义,它一定是有感而发的,自由的,自然的,不拘一格的。摇滚可以不批判,但如果完全屈服于市场,那它就是商品,有确定的价格,而没有其内在可挖掘的价值.有点岔开话题,这段访谈的我不全认同,毕竟我不是机器,再说机器也有一个处理单元,会将输入的东西进行转化一下或稍作处理进行输出,哪怕一个简单的函数也是这样. 互联网大潮将人们的言论冲散在各个地方,随处可见.其中我们不妨将一个观点看做一个岛屿,你只看到有的岛屿上人多,有的人少.有的人们会跟随某个具有代表性的人去向某个岛,也有的会搭着小船一下到这个岛,一下空降到另一个岛.这种情景我是不希望看到的,作为一个人,一个个体,他有自己的处理单元,他应该做到可以独立思考,而不是跟随着舆论的大潮漂来漂去,你应该在某个合理的范围内坚守自己的观点,将这个岛屿看做你的一个暂时家园,请不要轻易放弃它. 哦对了,观点的表达来自话语,话语不能表达一个观点的全部.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[借天池中间件复赛谈NIO]]></title>
      <url>%2F2016%2F08%2F15%2Ftianchi-nio%2F</url>
      <content type="text"><![CDATA[比赛的简单总结参加天池是被裴师兄安利的,这个很重要.在我眼中,裴博代码一直是写得飞起的那种,就是一边用B站放着billboard榜单,一边在那啪啪啪敲着代码,这看起来绝对很酷.所以我内心是崇拜他的,至少在技术上,他很刻苦,表现在很多方面.在此不一一赘述,因为我并不想在这开表彰会.他告诉我参加这个比赛会收获很多,我当然无条件相信,他当时参加的是机器学习相关的比赛.上半年正好有一场预测音乐播放量的比赛,我便屁颠颠地参加,对机器学习一窍不通,于是便慢慢学,特征,建立模型,训练.耗时一个半月,纯模型效果并不好,于是加了一些规则,后来还用了股票里经常遇到的时序模型,嗯,挺高大上的.这个比赛的确学习了很多,但总感觉浮于表面,略显浮夸,写代码也不是很多,更多的参赛选手都是搞机器学习和数学统计的,我并不在行. 这时候一个新比赛来华科路演,跑过去听了一听,意料之中,没什么太大收获,但是这个比赛引起了我极大的兴趣.首先它涉及系统设计,代码量跟机器学习类的比赛肯定不是一个级别,初赛和复赛考察内容很不一样.我觉得这种比赛还是很有趣的,但可能比较累,因为代码一多,因此而诞生的bug也会多起来,这点不太像机器学习的比赛,大部分时间是在等待和思考策略,动手的时间并不多. 中间件比赛分为初赛和复赛.初赛是实现一个实时统计双十一交易数据的系统,用到了三个组件jstorm,rocketmq和tair.都是阿里自家的产品,但都可以在原先的开源平台中找到原型,分别是storm,rabbitmq和redis.用法和api也大同小异,这个题目考察的主要还是熟悉这三个系统的使用,基本程序能正常运行跑出结果就能进前100了,如果想要跑到更好的成绩就需要想到更好的缓存策略.但你又不能把所有数据常驻内存,这样程序就会挂掉,而且这也不符合流计算的思想,流计算就应该是将数据想象成一段水流(不免让人想起破坏之王里的断水流大师兄),只不过水流会途径一些地方经过加工,如果水流囤积,那必然引起溃堤等风险.我的基本策略是jstorm的bolt主线程处理数据并统计,再开一个独立的线程用于数据的同步然后发送到下一个bolt.对我来说jstorm和storm最不同的一点就是:jstorm的Spout nextTuple和ack/fail运行在不同线程,这样可以防止CPU空转.storm的bolt和spout组件构成一个topology,一个设计优雅的拓扑图也是可以大大提高程序性能的. 关于复赛,设计一个订单查询系统,不在现有的开源平台上写应用,而是纯用java标准库,这个代码量可想而知.但后来赛方说可以用一些简单的轮子,比如基于磁盘的map,或者B+树什么的,纯用别人的开源数据库肯定是不行的.但比赛核心并不是这个,如果是这样功能大家都可以做出来,最重要的还是策略和优化.起先在github上找基于磁盘的B+树轮子,找了半天找到一个外国小伙写得轮子,很简洁.拿来一用,发现一个bug还有些许缺陷,后来还提交了commit给他,他也很客气merge了,不小的成就感,不得不说. 简单说一下程序策略: 构造：采用多线程分别对各个订单文件建立索引，索引内容为记录在文中的偏移量和该记录长度的结合体,起初先合并所有的订单文件再建立索引，后来发现合并与否对查询速度没有太大影响，由于没有进行多次测试，这个结论可能不成立。建立索引的方式起初是一部分B+树，一部分采用基于磁盘的map，后期由于B+树建立索引较慢，经常一小时内不能建完，后期全部改为基于disk的map。比赛后期性能改进包括将卖家和商品信息的索引全部放置在内存中，因为这两部分信息的索引大小之和并不大；将卖家与商品信息出现的字段缓存起来，若后期查询中字段不在其中，则省略了部分查询的步骤。 查询：针对每个索引上层封装为一个DB，其中包含多个table对象，对应各个小索引文件。在构造索引期间对每个原记录文件和索引文件分别映射为一个MappedByteBuffer对象，有的记录长度大于int最大值，对这种文件进行分段映射多个mappedbytebuffer对象，具体查询请求时，在bytebuffer中读取信息。查询支持完全并发，但是由于每个索引文件都对应了一个table所以查询时需要遍历所有table最后返回结果，这种策略较为愚蠢。 关于NIO这次比赛算是很好的一次理解和使用NIO的机会,借这次简单一下介绍下这方面.//未完待续]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[重写博客]]></title>
      <url>%2F2016%2F08%2F08%2Ffirst%2F</url>
      <content type="text"><![CDATA[之前也曾试图在网页写博客，但由于觉得博客网站形式大过实际意义就没怎么更新过。不过偶然的机会我发现，形式往往可以给我更多的动力或者动机去写东西。而且开始意识到写东西是真的可以带来乐趣的，我不想有一些想法过上一段时间就在我脑中死去，我得想办法把它记录下来。而这是一个很不错的办法。我不会只写技术内容，也会写一些心中所想和灵感，即使不能全部“刻录”下来，能写点总比不写好嘛。你说是吧。]]></content>
    </entry>

    
  
  
</search>
