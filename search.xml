<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[偶遇超级僵尸进程]]></title>
      <url>%2F2016%2F11%2F28%2Fsuper-zombie%2F</url>
      <content type="text"><![CDATA[你怎么死来死去都死不了啊？ 其实我差点就死了，你再给我多一点点时间，我就死定了。 ----喜剧之王 写在前面的话僵尸进程是经常遇到的，解决起来也比较方便。但一个偶然的机会遇到了一个父进程为1号进程的的僵尸进程，这个父进程肯定是不能随意杀死的，否则会导致严重的后果，至于什么严重的后果我也没测试过，搜了一下好像也没人细说。所以我私自地给它命名为超级僵尸进程，听起来给人一种很厉害的感觉，但又略带有一些调侃，加上上面的一句喜剧之王的台词，显得饶有趣味，好吧，岔开了，进入正题。 关于普通僵尸进程首先它是如何产生的？一个进程终止的方法很多，进程终止后有些信息对于父进程和内核还是很有用的，例如进程的ID号、进程的退出状态、进程运行的CPU时间等。因此进程在终止时回收所有内核分配给它的内存、关闭它打开的所有文件等等，但是还会保留以上极少的信息，以供父进程使用。父进程可以使用 wait/waitpid 等系统调用来为子进程收拾，做一些收尾工作。因此，一个僵尸进程产生的过程是：父进程调用fork创建子进程后，子进程运行直至其终止，它立即从内存中移除，但进程描述符仍然保留在内存中（进程描述符占有极少的内存空间）。子进程的状态变成EXIT_ZOMBIE，并且向父进程发送SIGCHLD信号，父进程此时应该调用wait()系统调用来获取子进程的退出状态以及其它的信息。在 wait 调用之后，僵尸进程就完全从内存中移除。因此一个僵尸存在于其终止到父进程调用wait等函数这个时间的间隙，一般很快就消失，但如果编程不合理，父进程从不调用 wait 等系统调用来收集僵尸进程，那么这些进程会一直存在内存中.概述来说就是两点原因：1.子进程终止后向父进程发出SIGCHLD信号，父进程默认忽略了它；2.父进程没有调用wait()或waitpid()函数来等待子进程的结束。如何发现系统中的僵尸进程？可以运行命令–&gt;ps -aux |grep -w ‘Z’ (这里的-w参数表示精确匹配) 如何杀掉普通僵尸进程把父进程杀掉，僵尸进程会变成孤儿进程，然后过继给1号进程，而1号进程会扫描名下子进程，把 Z 状态进程回收；这时候僵尸进程已经退出了，只保留了task_struct结构体，所以发信号（-9等信号）去处理僵尸进程是无效的； 超级僵尸进程通过对普通僵尸进程的分析,这样看起来好像父进程为1号进程的进程不会成为僵尸进程了，因为1号进程都会时刻扫描其子进程的状态，发现是僵尸进程就会马上去释放它的资源。但是，父进程为1号进程的进程 其实也是有可能成为僵尸进程的。下面说几种情况：1、进程还在被其它进程使用，退出；2、进程的子线程还在执行任务，但主线程已经死掉了（可能主线程已经被杀了，systemd停止服务时会发SIGTERM信号）；3、进程阻塞在某一IO请求上,这时控制权已交到内核手上,这时如果子进程被KILL掉,那么就成为父进程ID为1的僵尸进程,这个进程不会退出,会一直阻塞直到IO请求被满足。 应对超级僵尸进程其实ppid=1的僵尸进程可以不用去管他,因为它迟早会被1号进程回收的如果有很多僵尸进程除外.并且绝大部分ppid=1的僵尸进程是暂时的：1、当进程被跟踪调试完，则会自动被回收掉的；2、其他子线程组的线程执行完后会自动退出，僵尸进程会被回收；3、这个可能会一直挂着，如果阻塞的io永远没有到达； 总之，遇到少量的僵尸进程，可以不需要特意的去处理，只需要查看下根源，看看是否有潜在的bug就可以；]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论jvm优化]]></title>
      <url>%2F2016%2F11%2F26%2Fjava-optimize%2F</url>
      <content type="text"><![CDATA[一些要交代的假设你是一个普通的 Java 对象]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[科学配置hive+mysql+hadoop]]></title>
      <url>%2F2016%2F11%2F24%2Fhive-mysql%2F</url>
      <content type="text"><![CDATA[mysql配置先前的服务器mysql有一些莫名地故障，于是准备重装。贸然重装肯定是一件愚蠢的事，首先要做的是把原有的mysql卸载干净。但如果你的mysql能正常地工作，那当我没提起这件事。卸载遵循以下步骤： *运行rpm -qa|grep -i mysql。 显示如下： *删掉它们，当然之前要停止mysql服务，运行rpm -e --nodeps 包名 *查找之前老版本mysql的目录、并且删除老版本mysql的文件和库,运行find / -name mysql，会出现很多关于mysql的目录，删掉它们。 *run "rm -rf /etc/my.cnf" *再次查找机器是否安装mysql --&gt;rpm -qa|grep -i mysql 接下来安装mysql，方便起见，采用rpm的安装方式。下载mysql全家桶下载地址，然后解压，安装=》sudo rpm -ivh MySQL-*。顺便转一下配置文件=》sudo cp /usr/share/mysql/my-large.cnf /etc/my.cnf。 开启mysql服务=》 service mysql start。 设置用户密码=》 sudo /usr/bin/mysqladmin -u root password ‘123’ 进入mysql=》mysql -uroot -p123 创建mysql的hive用户=》 CREATE USER ‘hive’@’localhost’ IDENTIFIED BY “123” 创建数据库=》create database hive; 赋予权限=》 grant all on hive. to hive@’%’ identified by ‘hive’; grant all on hive. to hive@’localhost’ identified by ‘hive’; flush privileges; 退出mysql顺便可以验证下hive用户=》mysql -uhive -phive hadoop安装略不过注意一个问题：之前hadoop start起来没问题，也没有仔细看日志，但是运行hadoop命令总是遇到connection refused的问题，之前以为防火墙的原因。后来发现是用户bashrc的环境配置默认hadoop是另一个版本的，所以hadoop的命令工具来自另个hadoop。这个错误原因有点离谱，但还是挺难发现的。 hive安装与配置安装配置首先下载 hive1.2.1=》 nohup wget http://mirrors.hust.edu.cn/apache/hive/stable/apache-hive-1.2.1-bin.tar.gz &amp;解压 tar -zxvf ..配置环境变量 .bashrc=&gt;export HIVE_HOME=/home/feiwang/hive-2.0export HIVE_CONF_DIR=/home/feiwang/hive-2.0/confexport PATH=$PATH:$HIVE_HOME/binsource .bashrc修改hive配置文件，将默认文件mv成hive-site.xml，不是hive-default.xml！之前一直读取配置有问题就是这个原因。主要修改以下参数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL &lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive &lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName &lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver &lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword &lt;/name&gt; &lt;value&gt;hive &lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.hwi.listen.port &lt;/name&gt; &lt;value&gt;9999 &lt;/value&gt; &lt;description&gt;This is the port the Hive Web Interface will listen on &lt;/descript ion&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://127.0.0.1:9083&lt;/value&gt; &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: $&#123;hive.exec.scratchdir&#125;/&amp;lt;username&amp;gt; is created, with $&#123;hive.scratch.dir.permission&#125;.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/hive/local&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/tmp/hive/resources&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;&lt;/property&gt; 下载jdbc的jar包=》wget http://115.156.188.231/cdn.mysql.com//Downloads/Connector-J/mysql-connector-java-5.1.40.tar.gz 拷贝mysql-connector-java-5.1.40-bin.jar 到hive 的lib下面mv mysql-connector-java-5.1.40-bin.jar /root/feiwang/hive-2.0/lib/ 启动元数据 =》bin/hive –service metastore &amp; 验证运行 hive命令,创建表：进入mysql会发现hive数据库自动生成了很多表进入hadoop的hdfs会发现也有两个文件夹生成了Found 2 itemsdrwxr-xr-x - root supergroup 0 2016-11-24 21:43 /user/hive/warehouse/diablodrwxr-xr-x - root supergroup 0 2016-11-24 19:45 /user/hive/warehouse/kzx 简单介绍下hiveHive是运行在Hadoop之上的数据仓库，将结构化的数据文件映射为一张数据库表，提供简单类SQL查询语言，称为HQL，并将SQL语句转换成MapReduce任务运。有利于利用SQL语言查询、分析数据，适于处理不频繁变动的数据。Hive底层可以是HBase或者HDFS存储的文件。两者都是基于Hadoop上不同的技术，相互结合使用，可处理企业中不同类型的业务，利用Hive处理非结构化离线分析统计，利用HBase处理在线查询。Hive三种元数据存储方式：1&gt;.本地derby存储，只允许一个用户连接Hive，适用于测试环境2&gt;.本地/远程MySQL存储，支持多用户连接Hive，适用于生产环境]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[什么样的Stephen Chow]]></title>
      <url>%2F2016%2F11%2F11%2Fstephen-chow%2F</url>
      <content type="text"><![CDATA[我一直认为,在我进入大学后出现了两位我人生中的精神导师,这个词意义说起来挺重大的,但我的确是认真的说出了这句话.他们一位是作家,一位是演员.他们的名字叫做王小波和周星驰.//待更新,看心情]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在大菠萝中遇到的一些小问题]]></title>
      <url>%2F2016%2F10%2F10%2Fdiablo%2F</url>
      <content type="text"><![CDATA[大菠萝的任务大菠萝全名diablo technology,是一家硬件技术外企.但是我是一名纯软界的小小码农,怎么会帮一个硬件公司做事呢.起初是这家公司中国区的技术人员跟我们组里的陆博大大比较熟,他们想招一两个懂Spark的人帮他们做一些测试和性能上的优化来得出他们研发的新硬件M1相比于市场上普通内存的特性与性能差异.于是陆博建议我担任这份兼职,工作不多而且还可以增一份不错的收入,众所周知,这种事情我是无法拒绝的. 为了推广M1,他们需要在M1上运行很多工业市场上流行的吃内存的软件与平台,包括spark,mysql,redis等.所以这三个月我就没事帮他们写一点测试的代码,优化spark的配置,寻找运行错误的bug,搭建redis-cluster这种活.期间爬过很多坑,有几个瞬间我感觉到自己是技术顾问,让我小小地膨胀一下. C程序的测试C语言我是有好几年没碰了,作为一个大四以后一直在jvm语言中游走,有时用python写点小项目的人,几乎对指针这种东西处于懵逼模式. 代码的编写和测试流程我就不详细说了,说说遇到的段错误解决方案.也是这次跑C程序让我有了了解这方面知识的契机.发生了段错误/core dump,总结一下原因,一般是这几个: 访问了不存在的内存地址 访问了系统保护的内存地址 访问了只读的内存地址等等情况 先用gdb -g -o xx xx.c 生成gdb可调试的文件。说到gdb调试,大概要知道这几个命令: gdb xx 进入gdb调试界面 b 行数 在第多少行打断点 按r进入运行状态 n是单步调试 s进入函数中调试 q是退出 继续说解决core dump的bug, 然后运行程序, ./xx发生段错误后会生成core文件.接着运行命令gdb xx core.36129.输入where 会打印出具体的core dump发生在代码中哪个位置，方便定位bug。 还有一个bug是,每次都在内存地址被free的时候报无效指针,这种情况一般包含以下几个原因: 一个地址被free了两次, 当然这个很容易查出来. free的地址已经不是当时申请内存的起始地址了,注意可能在程序中的一些函数中对其做了一些操作. Spark-Sql测试框架中踩得坑大菠萝公司的喵叽(算我们的上司大人)想在机器上测试Spark Sql在M1上跑的性能，让我调研下Spark-sql-perf这个已有的测试框架(项目地址)。看了一下框架代码，差不多就是写了很多sql语句封装成一个个benchmark对象然后有一个多线程模式，然后一起开测。首先，在大菠萝的机器上部署测试框架，然后写一个简单的测试程序。代码如下:1234567891011121314151617181920212223object SqlTest extends Serializable&#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName("test") .setMaster("local") val sc = new SparkContext(sparkConf) val sqlContext = new org.apache.spark.sql.SQLContext(sc) val tables = new Tables(sqlContext, "/Users/iceke/projects/tpcds-kit/tools", Integer.parseInt("1")) tables.genData("data","parquet",true, false, false, false, false) val tableNames = Array("call_center", "catalog_page", "catalog_returns", "catalog_sales", "customer", "customer_address", "customer_demographics", "date_dim", "household_demographics", "income_band", "inventory", "item", "promotion", "reason", "ship_mode", "store", "store_returns", "store_sales", "time_dim", "warehouse", "web_page", "web_returns", "web_sales", "web_site") for(i &lt;- 0 to tableNames.length - 1) &#123; val a = sqlContext.read.parquet("data" + "/" + tableNames&#123;i&#125;) // sc.broadcast(a) a.registerTempTable(tableNames&#123;i&#125;) &#125; val tpcds = new TPCDS (sqlContext = sqlContext) val experiment = tpcds.runExperiment(tpcds.tpcds1_4Queries, iterations = 1,forkThread=false) experiment.waitForFinish(60*60*10) &#125; 先生成数据，大概几十张表。然后加载这几十张表，进行benchmark实验。本来过程很简单，但是需求是无止境的，喵叽说试试把所有的表cache到内存中，这样进行查询时快一点，听上去非常简单，在注册后加上一行cache表的代码就可以了。部分代码如下：1234 val a = sqlContext.read.parquet("data" + "/" + tableNames&#123;i&#125;)// sc.broadcast(a) a.registerTempTable(tableNames&#123;i&#125;) sqlContext.cacheTable(tableNames&#123;i&#125;) 我在mac上用local模式测了一下ok便提交给喵叽。 事情永远不会像想象的那么顺利，喵叽在集群上一测，就通过了几个benchmark，然后大量的stage报错，全部都是failed to get broadcast(TorrentBroadcast)异常，然后stage 直接失败，查看executor的日志发现已经有的broacast被remove了，但是接下来的加个task又会去获取这些broadcast，便会直接失败。这明显是不符合逻辑的，明明cache了所有表，靠异常堆栈信息并不好定位到错误的地方。我看了spark主页发现storage页面cache的RDD过一段时间就会消失，这也是一个重要的线索。 测试了半天还是出错，我开始静下心慢慢跟着框架的代码走，我发现最后每一个线程都会执行一个doBenchmark方法，方法里面最后执行完查询操作之后会做一些善后处理。相关代码：1234567891011121314151617final def benchmark( includeBreakdown: Boolean, description: String = "", messages: ArrayBuffer[String], timeout: Long, forkThread: Boolean = true): BenchmarkResult = &#123; logger.info(s"$this: benchmark") sparkContext.setJobDescription(s"Execution: $name, $description") beforeBenchmark() val result = if (forkThread) &#123; runBenchmarkForked(includeBreakdown, description, messages, timeout) &#125; else &#123; doBenchmark(includeBreakdown, description, messages) &#125; println("!!!!!!!!!!!!!!!!after Bench") afterBenchmark(sqlContext.sparkContext) result 这个善后处理是afterBenchmark方法，方法代码：12345678private def afterBenchmark(sc: SparkContext): Unit = &#123; // Best-effort clean up of weakly referenced RDDs, shuffles, and broadcasts System.gc() // Remove any leftover blocks that still exist sc.getExecutorStorageStatus .flatMap &#123; status =&gt; status.blocks.map &#123; case (bid, _) =&gt; bid &#125; &#125; .foreach &#123; bid =&gt; SparkEnv.get.blockManager.master.removeBlock(bid) &#125;&#125; 它会对每个相关block的id进行移除，无论它是否做了cache。所以解决方法是注释掉afterBenchmark方法，与上次堆外内存的bug一样，发现bug的过程是痛苦的，解决方案是简单到发指的,让人痛苦,又让人快乐. 所以说，别人的框架不是万能的，虽然你操作起来更便捷，但你必须遵守它制定的那一些规则，有些规则是操蛋的，当你使用它时，如果一直被操蛋的bug所围困，就需要看看它的源码看看是否有问题，或者与你的策略存在偶然性的冲突。 最后, 我的梦想是成为规则的制定者.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[修改Spark内核(Deca)所产生的两个奇怪bug]]></title>
      <url>%2F2016%2F09%2F28%2Fspark-bug%2F</url>
      <content type="text"><![CDATA[Spark将数据放在jvm堆外导致无故卡死思路实现与问题产生咱们内存计算小组陆博的文章Deca经过一年的努力终于中了vldb,作为其中参与了一些微小的工作的男人,虽然对其中一部分细节不是太清楚,因为这个Deca系统的开发涉及到将近5个人,我的工作囊括一下就是: 1.完成了其中UDF的转换,将方法的操作转为对字节数组的操作; 2.手写了Deca的手动版的代码,就是利用Deca的思想对Spark应用的代码进行改造; 3.进行了大量的测试并统计GC时间,stage时间等相关数据. 其实Deca系统的核心思想就是将原有的java大对象转化为字节数组有序地放置在jvm中,这样一可以减少对内存的使用,也可以基本避免所有的GC.附上论文Deca论文地址 老板的top中了之后,当然会将它扩展扩展然后投期刊,这是基本套路.要求扩展30%,其中就包括将之前的手动版的数据放置在堆外,还是以字节数组的形式来和堆内版本进行比较,理论上来说堆外版本肯定性能是比堆内好的,毕竟放置在堆外可以完全逃避GC的控制,也更加符合Deca的思想. 代码实现并不难,基本由Unsafe这个类操作完成.大概思路就是将Spark应用中需要缓存的RDD其中的partition的对象用字节数组的形式写在堆外,读的时候再直接按照偏移量读取,贴上部分代码:1234567891011import UnsafePR._private val baseAddress = UNSAFE.allocateMemory(size)private var curAddress = baseAddressdef address = baseAddressdef free:Unit=&#123; UNSAFE.freeMemory(baseAddress)&#125;def writeInt(num:Int):Unit=&#123; UNSAFE.putInt(curAddress,num) curAddress += 4&#125; 完整的Deca手动版PageRank代码地址 起先在本机的local模式测试了堆外版LR和PageRank应用当然是没问题的,结果也是正确的.然后转移到服务器集群上进行测试.令人惊喜的是,一个神奇的bug出现了. Bug的特征此Bug是本人coding以来见识到的算的上奇怪的一个bug了,它有以下几个特征.首先LR的堆外版本集群测试是没有问题的,但是切换到PageRank的堆外版本来测试时,总是在ZipPartition这个stage最后几个task执行的时候jvm crash掉,这个job就直接卡死了,必须手动杀掉才能停止.executor异常日志: stage卡住图示: 而且还有一个特殊的症状:就是PR跑2G数据量的时候居然不会挂掉,一到7G和20G的时候就会挂掉.而且local模式不会出错,一个executor也不会出错,一旦增加到多个executor就会出错. Bug原因分析与结论一开始想到的原因是shuffle的问题,因为local和单个executor不会出错,一旦涉及到网络传输就会报错.我怀疑是不是序列化方式的问题,分别用kryo和java自带的序列化方式测试了一下,然而都会报错.后来想了想应该不是网络传输的问题,不然小数据量怎么可以通过.我上网查了一下jvm crash那段报错信息,基本都是由于Unsafe访问到非法位置的原因,于是开始往这个方向考虑. 最后与师兄讨论中意识到问题的关键所在,首先Spark一个executor执行task比较慢时,如果另一个executor执行完一个stage的所有task时,会将剩余的task调度到那个节点去执行,也就是non-local task.non-local task是从网络传输过去的,这部分task是由cache RDD的partition生成而来的,这部分task是从block manager过去的,然而partition中的UnsafeEdge对象中只有一个Unsafe成员变量,一个初始地址和终点地址,和分配在jvm堆内的对象不同,并不携带真正的数据.所以这个task被调度到其他executor时,自然会非法访问堆外内存,然后jvm crash掉,这也可以解释为什么stage中位置为Any的task都不能成功执行这个现象.至于2G的数据量为什么可以通过,因为task运行的时间很短,几乎不需要调度就可以在一个executor中全部完成. 所以解决方案就是尽量不让task调度到其他的executor上执行,可以尽量增大spark.locality.wait这个变量来避免出错. 硕大的cache数据这个就简洁地介绍一下了,这是我大概11月10号遇见的 背景关于shuffle的VST拆解部分需要加到论文修改中,不过在意外中我发现了之前Deca release1.0版本的一个bug.那就是由于Deca将cache数据的对象完全转化为字节数组存储在jvm中,但是吊诡的现象在于Deca cache的数据居然比原生Spark的还大,这明显是不合理的.初看了一下代码发现是没有问题的.于是联系已经毕业的裴师兄,他说他之前就遇到这个问题了,只不过一直没改.这应该就是传说中的前人挖坑,后人填坑.但我同时也是很兴奋的,因为我们之前手动版本的实验结果很合理,改动Spark内核的自动版本也是同样地思路和流程,但是却出现了这种奇怪的现象,你知道解决bug是一个很能产生成就感的一个举措.在SparkContext将cache的RDD的迭代器做了一次调整,生成一个新的RDD并cache,然后将原本cache的RDD释放掉,重新调整一下RDD链,这样缓存的RDD将会被我们生成的RDD替换掉.之前对缓存的RDD所做的操作是:将里面返回KV对的迭代器改写一下,变换成往一个字节数组缓冲区写字节数组(按顺序写),然后返回新的迭代器. 修复后来发现之前返回的迭代器基本单位还是一个个KV对,这样就算是按字节数组写了也还是和Spark Cache的数据占用着差不多的大小,于是很简单啦,这里我将一块字节数组区称为CacheChunk,事实上它的类名也是这个.生成CacheChunk后,用Iterator包装一下便返回.跑了一下local模式,结果可想而知,抛出血红色的异常,我用的Idea,不知道其他的编辑器是不是这样.众所周知,Spark的每一个stage的结束要么是ShuffleTask,要么是ResultTask.ShuffleTask需要落磁盘,往block写点什么,这时候出发真正的RDD计算,就是调用RDD的迭代器.当然这里有个判断,如果某个RDD被定义了persist,第一次计算时会将它的计算结果常驻在内存中再返回迭代器,这样下一个stage用到此RDD时便直接在内存中获取,无需计算.管理这个流程的是一个叫CacheManager的哥们,我们之前返回的迭代器是一个iterator,iterator里包含着CacheChunk,CacheChunk里面又有一个迭代器方法,所以CacheManager肯定识别不了了呗.方便,给这个变换的CacheRDD加一个标签变量,切勿不要给它trasient这个标识,它需要被序列化.如果是Deca的RDD便让CacheManager多一个步骤,获取迭代器之后,强制转换为CacheChunk,再取一次迭代器,这样就能获取真正的数据.好,测试通过,perfect.自信地放到集群中去测试,令人”欣慰”的是,cache的数据反而更巨大! 后来怎么办呢,研究CacheManager下面的代码,继续调试往里面跟着走,我只想感叹一句,真的很深…就连一个普通的HashMap Spark都会根据自己的需求改.最后发现RDD的大小的评估是一个SizeEstimator的类实现的,部分功能代码如下: 12345678910111213141516private def visitSingleObject(obj: AnyRef, state: SearchState) &#123; val cls = obj.getClass if (cls.isArray) &#123; visitArray(obj, cls, state) &#125; else if (obj.isInstanceOf[ClassLoader] || obj.isInstanceOf[Class[_]]) &#123; // Hadoop JobConfs created in the interpreter have a ClassLoader, which greatly confuses // the size estimator since it references the whole REPL. Do nothing in this case. In // general all ClassLoaders and Classes will be shared between objects anyway. &#125; else &#123; val classInfo = getClassInfo(cls) state.size += alignSize(classInfo.shellSize) for (field &lt;- classInfo.pointerFields) &#123; state.enqueue(field.get(obj)) &#125; &#125;&#125; 大概思路就是判断对象类型,原生类型直接算,数组就累加算.如果不是原生类型,继续将里面的成员变量入栈,递归调用此函数.当我调试到这儿看调试信息的时候,发现一共访问了有一千多个成员变量,明显不合理.后来发现,CacheChunk里有个Spark定制的IterupptedIterator,这里面带出一批Spark相关的变量,导致评估大小大了整整十几倍.可是CacheChunk是可以不需要这个iterator的,于是将它换了个地方.重新测试,终于okay.流下了喜悦的眼泪,想高歌一曲,想起在实验室便作罢. 之后还遇到CacheChunk自动扩增容量的问题,不过解决起来比较简单,就不在此描述.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论思想同化的艺术]]></title>
      <url>%2F2016%2F09%2F11%2Fassimilation%2F</url>
      <content type="text"><![CDATA[看过一段访谈,一个摇滚乐队主唱的访谈,这个人叫邓斐,乐队名字叫寂寞.夏.日.曾经有一段时间疯狂地听他们的歌.他谈到他们的歌《这个易同化的民族》时说道: "我们正在被对权力的卑躬屈膝同化,正在被对文化和艺术弱智的认知同化,我们拿来从不消化甚至不知何物就充当自己, 我们做事不认真不努力随后从不自我反省反而学会相互制约和压制欺骗,我们正在被机会主义同化从无察觉..." 他想批判的很多,在其歌词中也可见一斑,他在我眼中算得上摇滚界的当代杜甫.这里要说到摇滚是什么,窦唯曾经说过:对于摇滚没有必要给它做个定义,它一定是有感而发的,自由的,自然的,不拘一格的。摇滚可以不批判,但如果完全屈服于市场,那它就是商品,有确定的价格,而没有其内在可挖掘的价值.有点岔开话题,这段访谈的我不全认同,毕竟我不是机器,再说机器也有一个处理单元,会将输入的东西进行转化一下或稍作处理进行输出,哪怕一个简单的函数也是这样. 互联网大潮将人们的言论冲散在各个地方,随处可见.其中我们不妨将一个观点看做一个岛屿,你只看到有的岛屿上人多,有的人少.有的人们会跟随某个具有代表性的人去向某个岛,也有的会搭着小船一下到这个岛,一下空降到另一个岛.这种情景我是不希望看到的,作为一个人,一个个体,他有自己的处理单元,他应该做到可以独立思考,而不是跟随着舆论的大潮漂来漂去,你应该在某个合理的范围内坚守自己的观点,将这个岛屿看做你的一个暂时家园,请不要轻易放弃它. 哦对了,观点的表达来自话语,话语不能表达一个观点的全部.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[借天池中间件复赛谈NIO]]></title>
      <url>%2F2016%2F08%2F15%2Ftianchi-nio%2F</url>
      <content type="text"><![CDATA[比赛的简单总结参加天池是被裴师兄安利的,这个很重要.在我眼中,裴博代码一直是写得飞起的那种,就是一边用B站放着billboard榜单,一边在那啪啪啪敲着代码,这看起来绝对很酷.所以我内心是崇拜他的,至少在技术上,他很刻苦,表现在很多方面.在此不一一赘述,因为我并不想在这开表彰会.他告诉我参加这个比赛会收获很多,我当然无条件相信,他当时参加的是机器学习相关的比赛.上半年正好有一场预测音乐播放量的比赛,我便屁颠颠地参加,对机器学习一窍不通,于是便慢慢学,特征,建立模型,训练.耗时一个半月,纯模型效果并不好,于是加了一些规则,后来还用了股票里经常遇到的时序模型,嗯,挺高大上的.这个比赛的确学习了很多,但总感觉浮于表面,略显浮夸,写代码也不是很多,更多的参赛选手都是搞机器学习和数学统计的,我并不在行. 这时候一个新比赛来华科路演,跑过去听了一听,意料之中,没什么太大收获,但是这个比赛引起了我极大的兴趣.首先它涉及系统设计,代码量跟机器学习类的比赛肯定不是一个级别,初赛和复赛考察内容很不一样.我觉得这种比赛还是很有趣的,但可能比较累,因为代码一多,因此而诞生的bug也会多起来,这点不太像机器学习的比赛,大部分时间是在等待和思考策略,动手的时间并不多. 中间件比赛分为初赛和复赛.初赛是实现一个实时统计双十一交易数据的系统,用到了三个组件jstorm,rocketmq和tair.都是阿里自家的产品,但都可以在原先的开源平台中找到原型,分别是storm,rabbitmq和redis.用法和api也大同小异,这个题目考察的主要还是熟悉这三个系统的使用,基本程序能正常运行跑出结果就能进前100了,如果想要跑到更好的成绩就需要想到更好的缓存策略.但你又不能把所有数据常驻内存,这样程序就会挂掉,而且这也不符合流计算的思想,流计算就应该是将数据想象成一段水流(不免让人想起破坏之王里的断水流大师兄),只不过水流会途径一些地方经过加工,如果水流囤积,那必然引起溃堤等风险.我的基本策略是jstorm的bolt主线程处理数据并统计,再开一个独立的线程用于数据的同步然后发送到下一个bolt.对我来说jstorm和storm最不同的一点就是:jstorm的Spout nextTuple和ack/fail运行在不同线程,这样可以防止CPU空转.storm的bolt和spout组件构成一个topology,一个设计优雅的拓扑图也是可以大大提高程序性能的. 关于复赛,设计一个订单查询系统,不在现有的开源平台上写应用,而是纯用java标准库,这个代码量可想而知.但后来赛方说可以用一些简单的轮子,比如基于磁盘的map,或者B+树什么的,纯用别人的开源数据库肯定是不行的.但比赛核心并不是这个,如果是这样功能大家都可以做出来,最重要的还是策略和优化.起先在github上找基于磁盘的B+树轮子,找了半天找到一个外国小伙写得轮子,很简洁.拿来一用,发现一个bug还有些许缺陷,后来还提交了commit给他,他也很客气merge了,不小的成就感,不得不说. 简单说一下程序策略: 构造：采用多线程分别对各个订单文件建立索引，索引内容为记录在文中的偏移量和该记录长度的结合体,起初先合并所有的订单文件再建立索引，后来发现合并与否对查询速度没有太大影响，由于没有进行多次测试，这个结论可能不成立。建立索引的方式起初是一部分B+树，一部分采用基于磁盘的map，后期由于B+树建立索引较慢，经常一小时内不能建完，后期全部改为基于disk的map。比赛后期性能改进包括将卖家和商品信息的索引全部放置在内存中，因为这两部分信息的索引大小之和并不大；将卖家与商品信息出现的字段缓存起来，若后期查询中字段不在其中，则省略了部分查询的步骤。 查询：针对每个索引上层封装为一个DB，其中包含多个table对象，对应各个小索引文件。在构造索引期间对每个原记录文件和索引文件分别映射为一个MappedByteBuffer对象，有的记录长度大于int最大值，对这种文件进行分段映射多个mappedbytebuffer对象，具体查询请求时，在bytebuffer中读取信息。查询支持完全并发，但是由于每个索引文件都对应了一个table所以查询时需要遍历所有table最后返回结果，这种策略较为愚蠢。 关于NIO这次比赛算是很好的一次理解和使用NIO的机会,借这次简单一下介绍下这方面.//未完待续]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Some Pictures]]></title>
      <url>%2F2015%2F10%2F14%2FSome-Pictures%2F</url>
      <content type="text"></content>
    </entry>

    
  
  
</search>
